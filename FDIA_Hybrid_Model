{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10644071,"sourceType":"datasetVersion","datasetId":6590626},{"sourceId":10649988,"sourceType":"datasetVersion","datasetId":6594532}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rana2hin/fdia-final?scriptVersionId=227367429\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nmerged_data = pd.read_parquet('/kaggle/input/fdia-sample-dataset-simbench/merged_data.parquet')\nlines_sub = pd.read_parquet('/kaggle/input/fdia-sample-dataset-simbench/lines_sub.parquet')\n\nmerged_data.info()\nlines_sub.info()\n\nmerged_data.head()\nlines_sub.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-13T09:29:13.383029Z","iopub.execute_input":"2025-03-13T09:29:13.38328Z","iopub.status.idle":"2025-03-13T09:29:14.667328Z","shell.execute_reply.started":"2025-03-13T09:29:13.383248Z","shell.execute_reply":"2025-03-13T09:29:14.663955Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install spektral","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T09:29:14.668324Z","iopub.execute_input":"2025-03-13T09:29:14.66883Z","iopub.status.idle":"2025-03-13T09:29:20.31757Z","shell.execute_reply.started":"2025-03-13T09:29:14.668798Z","shell.execute_reply":"2025-03-13T09:29:20.316449Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ========================\n# Step 0: Import Libraries\n# ========================\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import (Input, Dense, LSTM, Dropout, Concatenate,\n                                     Reshape, Flatten, BatchNormalization,\n                                     LeakyReLU, Activation)\nfrom tensorflow.keras.optimizers import Adam\n\n# For the GNN, we use Spektral (install via: pip install spektral)\nfrom spektral.layers import GCNConv\n\n# For evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# ===============================\n# Step 1: Data Loading & Preprocessing\n# ===============================\n# Example: load your measurement and topology data (adjust file paths as needed)\nmeasurement_df = pd.read_csv('/kaggle/input/simbench-sample-data/measurements.csv', parse_dates=['time'])\ntopology_df = pd.read_csv('/kaggle/input/simbench-sample-data/topology.csv')\noriginal_df = measurement_df.copy()\n\n# FDIA simulation parameters\nattack_ratio = 0.3  # 30% of data is compromised\nmax_attack_strength = 0.4  # Maximum perturbation (40% deviation)\n\n# Generate attack targets\nn_samples = len(measurement_df)\nattack_indices = np.random.choice(n_samples, size=int(n_samples * attack_ratio), replace=False)\n\n# Simulate FDIA attacks\nfor idx in attack_indices:\n    # Randomly select feature to attack (pRES or value)\n    if np.random.rand() < 0.5:\n        # Attack pRES\n        perturbation = 1 + (np.random.rand() - 0.5) * 2 * max_attack_strength\n        measurement_df.loc[idx, 'pRES'] *= perturbation\n    else:\n        # Attack value\n        perturbation = 1 + (np.random.rand() - 0.5) * 2 * max_attack_strength\n        measurement_df.loc[idx, 'value'] *= perturbation\n    \n    # Recalculate p_t to maintain consistency\n    measurement_df.loc[idx, 'p_t'] = measurement_df.loc[idx, 'pRES'] * measurement_df.loc[idx, 'value']\n\n# Create labels: 1 for attacked samples, 0 for normal\ny = np.zeros(n_samples)\ny[attack_indices] = 1\n\n# Use attacked measurements for model input\nmeasurement_features = ['pRES', 'value', 'p_t']\nX_measure = measurement_df[measurement_features].values\n\n# Normalize using robust scaling (more resistant to outliers)\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_measure = scaler.fit_transform(X_measure)\n\n# For the GNN, you need a graph structure.\n# For example, create an adjacency matrix from topology_df. (This is highly domain dependent.)\n# Here we assume that each row in topology_df represents an edge between nodeA and nodeB.\n# We first build a mapping from node ID to index:\nnodes = pd.unique(topology_df[['nodeA', 'nodeB']].values.ravel())\nnode_to_index = {node: idx for idx, node in enumerate(nodes)}\nn_nodes = len(nodes)\n\n# Create a simple binary adjacency matrix\nadjacency = np.zeros((n_nodes, n_nodes))\nfor _, row in topology_df.iterrows():\n    i = node_to_index[row['nodeA']]\n    j = node_to_index[row['nodeB']]\n    adjacency[i, j] = 1\n    adjacency[j, i] = 1  # assume undirected graph\n\n# ===============================\n# Step 2: Build the Autoencoder (AE)\n# ===============================\ndef build_autoencoder(input_dim, encoding_dim=16):\n    input_layer = Input(shape=(input_dim,))\n    # Encoder\n    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n    # Decoder\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    encoder = Model(inputs=input_layer, outputs=encoded)\n    autoencoder.compile(optimizer='adam', loss='mse')\n    return autoencoder, encoder\n\ninput_dim = X_measure.shape[1]\n\nautoencoder, ae_encoder = build_autoencoder(input_dim, encoding_dim=16)\n\n# Train AE (use validation_split or separate test set as needed)\nautoencoder.fit(X_measure, X_measure, epochs=20, batch_size=32, validation_split=0.1)\n\n# ===============================\n# Step 3: Build the Graph Neural Network (GNN)\n# ===============================\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom spektral.layers import GCNConv\nimport numpy as np\n\nclass CustomGCNConv(GCNConv):\n    def compute_output_shape(self, input_shapes):\n        features_shape = input_shapes[0]\n        return (features_shape[0], features_shape[1], self.channels)\n        \n    def call(self, inputs, mask=None):\n        features, adjacency = inputs\n        output = super().call(inputs)\n        return output\n\ndef build_gnn(node_feature_dim, n_hidden=64, n_classes=16, n_nodes=None):\n    if n_nodes is None:\n        raise ValueError(\"n_nodes must be specified.\")\n        \n    X_in = Input(shape=(n_nodes, node_feature_dim))\n    A_in = Input(shape=(n_nodes, n_nodes), sparse=False)\n    \n    gc1 = CustomGCNConv(\n        n_hidden,\n        activation='relu',\n        kernel_initializer='glorot_uniform',\n        use_bias=True\n    )\n    gc1_out = gc1([X_in, A_in])\n    \n    gc2 = CustomGCNConv(\n        n_classes,\n        activation='relu',\n        kernel_initializer='glorot_uniform',\n        use_bias=True\n    )\n    gc2_out = gc2([gc1_out, A_in])\n    \n    model = Model(inputs=[X_in, A_in], outputs=gc2_out)\n    model.compile(\n        optimizer=Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae']\n    )\n    return model\n\ndef prepare_graph_data(node_features, adjacency, batch_size=1):\n    \"\"\"\n    Prepare graph data for training by adding batch dimension and normalizing adjacency matrix.\n    \"\"\"\n    from spektral.utils import normalized_adjacency\n    \n    # Ensure inputs are float32\n    node_features = node_features.astype(np.float32)\n    adjacency = adjacency.astype(np.float32)\n    \n    # Normalize adjacency matrix\n    adj_normalized = normalized_adjacency(adjacency)\n    \n    # Add batch dimension if not already present\n    if len(node_features.shape) == 2:\n        node_features_batch = np.expand_dims(node_features, axis=0)\n    else:\n        node_features_batch = node_features\n        \n    if len(adj_normalized.shape) == 2:\n        adj_normalized_batch = np.expand_dims(adj_normalized, axis=0)\n    else:\n        adj_normalized_batch = adj_normalized\n    \n    return node_features_batch, adj_normalized_batch\n\ndef example_usage(n_nodes=34, feature_dim=16):\n    # Create dummy data\n    node_features = np.random.randn(n_nodes, feature_dim).astype(np.float32)\n    adjacency = np.random.randint(0, 2, size=(n_nodes, n_nodes)).astype(np.float32)\n    # Make adjacency matrix symmetric\n    adjacency = np.maximum(adjacency, adjacency.T)\n    np.fill_diagonal(adjacency, 1)  # Add self-loops\n    \n    # Build model\n    model = build_gnn(\n        node_feature_dim=feature_dim,\n        n_hidden=32,\n        n_classes=feature_dim,\n        n_nodes=n_nodes\n    )\n    \n    # Prepare data\n    features_batch, adj_batch = prepare_graph_data(node_features, adjacency)\n    \n    # Create dummy targets (autoencoder-like reconstruction)\n    targets_batch = np.expand_dims(node_features, axis=0)\n    \n    # Train model\n    history = model.fit(\n        [features_batch, adj_batch],\n        targets_batch,\n        epochs=10,\n        batch_size=1,\n        verbose=1\n    )\n    \n    return model, history\n\n# Example usage\nn_nodes = 34\nfeature_dim = 16\ngnn_model, history = example_usage(n_nodes=n_nodes, feature_dim=feature_dim)\n\n# ===============================\n# Step 4: Build the RNN with Attention Mechanism (RNN+AM)\n# ===============================\n# Custom attention layer (suitable for many-to-one tasks)\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n        super(Attention, self).build(input_shape)\n    def call(self, x):\n        # Compute scores\n        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n        a = tf.keras.backend.softmax(e, axis=1)\n        output = x * a\n        return tf.keras.backend.sum(output, axis=1)\n\ndef build_rnn_attention(input_shape, lstm_units=64):\n    input_layer = Input(shape=input_shape)\n    lstm_out = LSTM(lstm_units, return_sequences=True)(input_layer)\n    attn_out = Attention()(lstm_out)\n    dense_out = Dense(128, activation='relu')(attn_out)\n    output = Dense(1, activation='sigmoid')(dense_out)\n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# For demonstration, reshape the measurement data into sequences.\n# (This example creates sequences of length 5; adjust based on your data.)\ntimesteps = 5\nif X_measure.shape[0] % timesteps != 0:\n    # trim data to form complete sequences\n    trim = X_measure.shape[0] - (X_measure.shape[0] % timesteps)\n    X_measure_seq = X_measure[:trim]\n    y_seq = y[:trim]\nelse:\n    X_measure_seq = X_measure\n    y_seq = y\n\nX_rnn = X_measure_seq.reshape(-1, timesteps, input_dim)\ny_rnn = y_seq.reshape(-1, timesteps)[:, -1]  # use the last label in each sequence as target\n\nrnn_model = build_rnn_attention(input_shape=(timesteps, input_dim), lstm_units=32)\nrnn_model.fit(X_rnn, y_rnn, epochs=10, batch_size=32, validation_split=0.1)\n\n# ===============================\n# Step 5: Build the Deep Belief Network (DBN) Module\n# ===============================\n# Revised: Use the Functional API for better flexibility and to define inputs properly.\ndef build_dbn(input_dim, hidden_dims=[64, 32], output_dim=1):\n    inputs = Input(shape=(input_dim,))\n    x = Dense(hidden_dims[0], activation='relu')(inputs)\n    for units in hidden_dims[1:]:\n        x = Dense(units, activation='relu')(x)\n    outputs = Dense(output_dim, activation='sigmoid')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ndbn_model = build_dbn(input_dim=input_dim, hidden_dims=[64, 32], output_dim=1)\ndbn_model.fit(X_measure, y, epochs=10, batch_size=32, validation_split=0.1)\n\n# ===============================\n# Step 6: Build the Generative Adversarial Network (GAN)\n# ===============================\n# GAN components: generator and discriminator.\nlatent_dim = 16  # dimension of the latent noise vector\noutput_dim = input_dim  # generating synthetic measurement feature vectors\n\ndef build_generator(latent_dim, output_dim):\n    model = Sequential()\n    model.add(Dense(128, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(output_dim, activation='tanh'))\n    return model\n\ndef build_discriminator(input_dim):\n    model = Sequential()\n    model.add(Dense(256, input_dim=input_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(128))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ngenerator = build_generator(latent_dim, output_dim)\ndiscriminator = build_discriminator(output_dim)\n\n# Build combined GAN model\ndef build_gan(generator, discriminator, latent_dim):\n    discriminator.trainable = False  # freeze discriminator during generator training\n    gan_input = Input(shape=(latent_dim,))\n    generated_sample = generator(gan_input)\n    gan_output = discriminator(generated_sample)\n    gan_model = Model(gan_input, gan_output)\n    gan_model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n    return gan_model\n\ngan_model = build_gan(generator, discriminator, latent_dim)\n\n# Train the GAN (simplified training loop)\nepochs = 1000\nbatch_size = 32\nfor epoch in range(epochs):\n    # --- Train Discriminator ---\n    # Select a random batch of real samples\n    idx = np.random.randint(0, X_measure.shape[0], batch_size)\n    real_samples = X_measure[idx]\n    # Generate fake samples\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    fake_samples = generator.predict(noise)\n    # Labels for real and fake data\n    d_loss_real = discriminator.train_on_batch(real_samples, np.ones((batch_size, 1)))\n    d_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((batch_size, 1)))\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n    \n    # --- Train Generator ---\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    g_loss = gan_model.train_on_batch(noise, np.ones((batch_size, 1)))\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}: [D loss: {d_loss[0]}, acc.: {d_loss[1]}] [G loss: {g_loss}]\")\n\n# ===============================\n# Step 5: Build the Deep Belief Network (DBN) Module\n# ===============================\n# Revised: Use the Functional API for better flexibility and to define inputs properly.\ndef build_dbn(input_dim, hidden_dims=[64, 32], output_dim=1):\n    inputs = Input(shape=(input_dim,))\n    x = Dense(hidden_dims[0], activation='relu')(inputs)\n    for units in hidden_dims[1:]:\n        x = Dense(units, activation='relu')(x)\n    outputs = Dense(output_dim, activation='sigmoid')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ndbn_model = build_dbn(input_dim=input_dim, hidden_dims=[64, 32], output_dim=1)\ndbn_model.fit(X_measure, y, epochs=10, batch_size=32, validation_split=0.1)\n\n# ===============================\n# Step 7: Build the Hybrid Model (Model Integration)\n# ===============================\n# --- Step A: Build Feature Extractors for RNN and DBN Branches ---\n\n# For the RNN branch, extract the 128-dimensional feature vector before the final classification.\ndef build_rnn_feature_extractor(rnn_model):\n    # Assumes rnn_model layers: [Input, LSTM, Attention, Dense(128, activation='relu'), Dense(1, activation='sigmoid')]\n    feature_extractor = Model(inputs=rnn_model.input, outputs=rnn_model.layers[-2].output)\n    return feature_extractor\n\nrnn_feature_extractor = build_rnn_feature_extractor(rnn_model)\n\n# For the DBN branch, extract the features from the penultimate layer.\ndef build_dbn_feature_extractor(dbn_model):\n    feature_extractor = Model(inputs=dbn_model.input, outputs=dbn_model.layers[-2].output)\n    return feature_extractor\n\ndbn_feature_extractor = build_dbn_feature_extractor(dbn_model)\n\n# --- Step B: Build the Enhanced Hybrid Model ---\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization, RepeatVector\n\ndef build_enhanced_hybrid(ae_encoder, rnn_feature_extractor, dbn_feature_extractor, input_dim, timesteps=5):\n    main_input = Input(shape=(input_dim,))\n    \n    # AE Branch: Extract encoded features (e.g., dimension 16)\n    ae_features = ae_encoder(main_input)  # shape: (None, 16)\n    \n    # RNN Branch: Create a sequence input and extract features (e.g., dimension 128)\n    rnn_input = RepeatVector(timesteps)(main_input)  # shape: (None, timesteps, input_dim)\n    rnn_features = rnn_feature_extractor(rnn_input)   # shape: (None, 128)\n    \n    # DBN Branch: Extract features (e.g., dimension 32 if hidden_dims=[64,32])\n    dbn_features = dbn_feature_extractor(main_input)    # shape: (None, 32)\n    \n    # Concatenate features from all branches\n    combined = Concatenate()([ae_features, rnn_features, dbn_features])\n    \n    # Fully connected layers for final discrimination\n    x = Dense(256, activation='relu')(combined)\n    x = Dropout(0.3)(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    output = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=main_input, outputs=output)\n    model.compile(\n        optimizer=Adam(learning_rate=0.0005),\n        loss='binary_crossentropy',\n        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n    )\n    return model\n\n# Build the hybrid model using the extracted feature branches\nhybrid_model = build_enhanced_hybrid(ae_encoder, rnn_feature_extractor, dbn_feature_extractor, input_dim, timesteps=5)\n\n# --- Step C: Train and Evaluate the Hybrid Model ---\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n\n# Stratified split to maintain class balance\nX_train, X_test, y_train, y_test = train_test_split(X_measure, y, test_size=0.2, \n                                                    stratify=y, random_state=42)\n\n# Dynamic learning rate scheduler\ndef lr_scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return float(lr * tf.math.exp(-0.1))\n\ncallbacks = [\n    EarlyStopping(patience=15, restore_best_weights=True),\n    LearningRateScheduler(lr_scheduler)\n]\n\n# Adjust class weights for imbalance\nclass_weights = {0: 1.0, 1: (len(y_train) - np.sum(y_train)) / np.sum(y_train)}\n\nhistory = hybrid_model.fit(\n    X_train, y_train,\n    epochs=100,\n    batch_size=256,\n    validation_split=0.2,\n    class_weight=class_weights,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# Evaluation: use default threshold 0.5 for classification\ny_pred_prob = hybrid_model.predict(X_test)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Normal', 'FDIA']))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T09:29:20.319324Z","iopub.execute_input":"2025-03-13T09:29:20.319637Z","iopub.status.idle":"2025-03-13T09:41:29.66313Z","shell.execute_reply.started":"2025-03-13T09:29:20.319609Z","shell.execute_reply":"2025-03-13T09:41:29.662218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom kerastuner import HyperModel, RandomSearch\n\n# --- 1. Fixing the Attention Layer ---\n@tf.keras.utils.register_keras_serializable()\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n        super(Attention, self).build(input_shape)\n    def call(self, x):\n        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n        a = tf.keras.backend.softmax(e, axis=1)\n        output = x * a\n        return tf.keras.backend.sum(output, axis=1)\n\n\n# --- 2. Focal Loss Definition (unchanged) ---\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        eps = 1e-8\n        y_pred = K.clip(y_pred, eps, 1. - eps)\n        pt = tf.where(tf.equal(y_true, 1), y_pred, 1-y_pred)\n        loss = -K.sum(alpha * K.pow(1-pt, gamma) * K.log(pt), axis=-1)\n        return loss\n    return focal_loss_fixed\n\n# Optionally, compile your hybrid_model (outside hypermodel) with focal loss:\nhybrid_model.compile(\n    optimizer=Adam(learning_rate=0.0005),\n    loss=focal_loss(gamma=2., alpha=.25),\n    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n)\n\n# --- 3. Create a function to get fresh callbacks ---\ndef get_callbacks():\n    return [\n        EarlyStopping(patience=15, restore_best_weights=True),\n        LearningRateScheduler(lambda epoch, lr: lr if epoch < 10 else float(lr * tf.math.exp(-0.1)))\n    ]\n\n# --- 4. HyperModel for Hyperparameter Tuning ---\nclass HybridHyperModel(HyperModel):\n    def build(self, hp):\n        # Adjust number of units, dropout rates, etc.\n        units = hp.Int('units', min_value=128, max_value=512, step=64)\n        dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n        \n        main_input = Input(shape=(input_dim,))\n        ae_features = ae_encoder(main_input)\n        rnn_input = RepeatVector(timesteps)(main_input)\n        rnn_features = rnn_feature_extractor(rnn_input)\n        dbn_features = dbn_feature_extractor(main_input)\n        combined = Concatenate()([ae_features, rnn_features, dbn_features])\n        \n        x = Dense(units, activation='relu')(combined)\n        x = Dropout(dropout_rate)(x)\n        x = BatchNormalization()(x)\n        x = Dense(units // 2, activation='relu')(x)\n        x = Dropout(dropout_rate / 2)(x)\n        output = Dense(1, activation='sigmoid')(x)\n        model = Model(inputs=main_input, outputs=output)\n        \n        # Optionally, you can compile with focal loss:\n        model.compile(\n            optimizer=Adam(learning_rate=0.0005),\n            loss=focal_loss(gamma=2., alpha=.25),\n            metrics=['accuracy']\n        )\n        return model\n\n# --- 5. Hyperparameter Search ---\nhypermodel = HybridHyperModel()\ntuner = RandomSearch(\n    hypermodel,\n    objective='val_accuracy',\n    max_trials=10,\n    executions_per_trial=2,\n    directory='hybrid_tuning',\n    project_name='fdia_detection'\n)\n\n# Note: Use get_callbacks() to pass new callback instances each time.\ntuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=get_callbacks())\nbest_model = tuner.get_best_models(num_models=1)[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T09:41:29.664292Z","iopub.execute_input":"2025-03-13T09:41:29.664561Z","iopub.status.idle":"2025-03-13T10:35:01.28839Z","shell.execute_reply.started":"2025-03-13T09:41:29.664539Z","shell.execute_reply":"2025-03-13T10:35:01.28738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred_prob = best_model.predict(X_test)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n# Print classification metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Normal', 'FDIA']))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:35:01.289592Z","iopub.execute_input":"2025-03-13T10:35:01.289986Z","iopub.status.idle":"2025-03-13T10:35:02.347312Z","shell.execute_reply.started":"2025-03-13T10:35:01.289955Z","shell.execute_reply":"2025-03-13T10:35:02.346313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n# Compute the confusion matrix using your test labels and predictions\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(6, 5))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Normal', 'FDIA'], rotation=45)\nplt.yticks(tick_marks, ['Normal', 'FDIA'])\n\n# Loop over data dimensions and create text annotations.\nthresh = cm.max() / 2.\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, format(cm[i, j], 'd'),\n                 ha=\"center\", va=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel(\"Actual Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:51:27.160559Z","iopub.execute_input":"2025-03-13T10:51:27.160973Z","iopub.status.idle":"2025-03-13T10:51:27.487327Z","shell.execute_reply.started":"2025-03-13T10:51:27.160943Z","shell.execute_reply":"2025-03-13T10:51:27.486346Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n\n# Compute ROC curve and AUC\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, label=\"ROC curve (AUC = %0.2f)\" % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', label=\"Chance\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\n\n# Compute Precision-Recall curve and average precision score\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\navg_precision = average_precision_score(y_test, y_pred_prob)\n\nplt.subplot(1, 2, 2)\nplt.plot(recall, precision, label=\"PR curve (AP = %0.2f)\" % avg_precision)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.legend(loc=\"lower left\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:54:47.662669Z","iopub.execute_input":"2025-03-13T10:54:47.663098Z","iopub.status.idle":"2025-03-13T10:54:48.117299Z","shell.execute_reply.started":"2025-03-13T10:54:47.663065Z","shell.execute_reply":"2025-03-13T10:54:48.116287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Loss curves\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Accuracy curves\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:52:32.567291Z","iopub.execute_input":"2025-03-13T10:52:32.567622Z","iopub.status.idle":"2025-03-13T10:52:33.027452Z","shell.execute_reply.started":"2025-03-13T10:52:32.567597Z","shell.execute_reply":"2025-03-13T10:52:33.026459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming y_pred_prob (predicted probabilities) and y_test (ground truth) are defined\n# Separate predicted probabilities for each class:\npreds_normal = y_pred_prob[y_test == 0]\npreds_fdia   = y_pred_prob[y_test == 1]\n\nplt.figure(figsize=(8, 5))\nplt.hist(preds_normal, bins=20, alpha=0.6, label='Normal', color='green')\nplt.hist(preds_fdia, bins=20, alpha=0.6, label='FDIA', color='red')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Predicted Probability Distribution by Class')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T10:59:34.569984Z","iopub.execute_input":"2025-03-13T10:59:34.570335Z","iopub.status.idle":"2025-03-13T10:59:34.862044Z","shell.execute_reply.started":"2025-03-13T10:59:34.57031Z","shell.execute_reply":"2025-03-13T10:59:34.861088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\n\n# Compute calibration curve: fraction of positives vs mean predicted probability\nprob_true, prob_pred = calibration_curve(y_test, y_pred_prob, n_bins=10)\n\nplt.figure(figsize=(8, 5))\nplt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:01:26.032472Z","iopub.execute_input":"2025-03-13T11:01:26.032863Z","iopub.status.idle":"2025-03-13T11:01:26.260679Z","shell.execute_reply.started":"2025-03-13T11:01:26.032822Z","shell.execute_reply":"2025-03-13T11:01:26.259554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Optionally suppress specific FutureWarnings from Seaborn\nwarnings.filterwarnings(\"ignore\", message=\".*use_inf_as_na.*\", module=\"seaborn\")\nwarnings.filterwarnings(\"ignore\", message=\".*When grouping with a length-1 list-like.*\", module=\"seaborn\")\n\n# Replace infinities with NaN before plotting\ndf_plot = measurement_df.replace([np.inf, -np.inf], np.nan).copy()\ndf_plot['Label'] = np.where(y==1, 'FDIA', 'Normal')\n\n# Plot distributions for each feature\nfeatures_to_plot = ['pRES', 'value', 'p_t']\nplt.figure(figsize=(15, 4))\nfor idx, feature in enumerate(features_to_plot):\n    plt.subplot(1, len(features_to_plot), idx+1)\n    sns.histplot(data=df_plot, x=feature, hue='Label', element='step', stat=\"density\", common_norm=False)\n    plt.title(f\"Distribution of {feature}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:09:19.012651Z","iopub.execute_input":"2025-03-13T11:09:19.013048Z","iopub.status.idle":"2025-03-13T11:09:19.871589Z","shell.execute_reply.started":"2025-03-13T11:09:19.013021Z","shell.execute_reply":"2025-03-13T11:09:19.870643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the correlation matrix for the measurement features\nfeatures_to_plot = ['pRES', 'value', 'p_t']\ncorr_matrix = measurement_df[features_to_plot].corr()\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap of Measurement Features')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-13T11:12:10.708009Z","iopub.execute_input":"2025-03-13T11:12:10.70835Z","iopub.status.idle":"2025-03-13T11:12:10.931046Z","shell.execute_reply.started":"2025-03-13T11:12:10.708326Z","shell.execute_reply":"2025-03-13T11:12:10.93014Z"}},"outputs":[],"execution_count":null}]}