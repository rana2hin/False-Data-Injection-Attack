{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10644071,"sourceType":"datasetVersion","datasetId":6590626},{"sourceId":10649988,"sourceType":"datasetVersion","datasetId":6594532}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rana2hin/fdia-final?scriptVersionId=238763616\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install spektral","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:07:01.090484Z","iopub.execute_input":"2025-05-09T12:07:01.090702Z","iopub.status.idle":"2025-05-09T12:07:04.586539Z","shell.execute_reply.started":"2025-05-09T12:07:01.090679Z","shell.execute_reply":"2025-05-09T12:07:04.585737Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: spektral in /usr/local/lib/python3.10/dist-packages (1.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from spektral) (1.4.2)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from spektral) (5.3.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from spektral) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from spektral) (2.2.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from spektral) (2.32.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from spektral) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.13.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spektral) (4.67.1)\nRequirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from spektral) (2.17.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.37.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2024.12.14)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2024.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->spektral) (3.5.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.2.0->spektral) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.2.0->spektral) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.2.0->spektral) (0.13.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.2.0->spektral) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.2.0->spektral) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.2.0->spektral) (3.1.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->spektral) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->spektral) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->spektral) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->spektral) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->spektral) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.2.0->spektral) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.2.0->spektral) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.2.0->spektral) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.2.0->spektral) (0.1.2)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ========================\n# Step 0: Import Libraries\n# ========================\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import (Input, Dense, LSTM, Dropout, Concatenate,\n                                     Reshape, Flatten, BatchNormalization,\n                                     LeakyReLU, Activation)\nfrom tensorflow.keras.optimizers import Adam\n\n# For the GNN, we use Spektral (install via: pip install spektral)\nfrom spektral.layers import GCNConv\n\n# For evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# ===============================\n# Step 1: Data Loading & Preprocessing\n# ===============================\n# Example: load your measurement and topology data (adjust file paths as needed)\nmeasurement_df = pd.read_csv('/kaggle/input/simbench-sample-data/measurements.csv', parse_dates=['time'])\ntopology_df = pd.read_csv('/kaggle/input/simbench-sample-data/topology.csv')\noriginal_df = measurement_df.copy()\n\n# FDIA simulation parameters\nattack_ratio = 0.3  # 30% of data is compromised\nmax_attack_strength = 0.4  # Maximum perturbation (40% deviation)\n\n# Generate attack targets\nn_samples = len(measurement_df)\nattack_indices = np.random.choice(n_samples, size=int(n_samples * attack_ratio), replace=False)\n\n# Simulate FDIA attacks\nfor idx in attack_indices:\n    # Randomly select feature to attack (pRES or value)\n    if np.random.rand() < 0.5:\n        # Attack pRES\n        perturbation = 1 + (np.random.rand() - 0.5) * 2 * max_attack_strength\n        measurement_df.loc[idx, 'pRES'] *= perturbation\n    else:\n        # Attack value\n        perturbation = 1 + (np.random.rand() - 0.5) * 2 * max_attack_strength\n        measurement_df.loc[idx, 'value'] *= perturbation\n    \n    # Recalculate p_t to maintain consistency\n    measurement_df.loc[idx, 'p_t'] = measurement_df.loc[idx, 'pRES'] * measurement_df.loc[idx, 'value']\n\n# Create labels: 1 for attacked samples, 0 for normal\ny = np.zeros(n_samples)\ny[attack_indices] = 1\n\n# Use attacked measurements for model input\nmeasurement_features = ['pRES', 'value', 'p_t']\nX_measure = measurement_df[measurement_features].values\n\n# Normalize using robust scaling (more resistant to outliers)\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_measure = scaler.fit_transform(X_measure)\n\n# For the GNN, you need a graph structure.\n# For example, create an adjacency matrix from topology_df. (This is highly domain dependent.)\n# Here we assume that each row in topology_df represents an edge between nodeA and nodeB.\n# We first build a mapping from node ID to index:\nnodes = pd.unique(topology_df[['nodeA', 'nodeB']].values.ravel())\nnode_to_index = {node: idx for idx, node in enumerate(nodes)}\nn_nodes = len(nodes)\n\n# Create a simple binary adjacency matrix\nadjacency = np.zeros((n_nodes, n_nodes))\nfor _, row in topology_df.iterrows():\n    i = node_to_index[row['nodeA']]\n    j = node_to_index[row['nodeB']]\n    adjacency[i, j] = 1\n    adjacency[j, i] = 1  # assume undirected graph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:07:08.088956Z","iopub.execute_input":"2025-05-09T12:07:08.089241Z","iopub.status.idle":"2025-05-09T12:07:24.391713Z","shell.execute_reply.started":"2025-05-09T12:07:08.089218Z","shell.execute_reply":"2025-05-09T12:07:24.390546Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# ===============================\n# Step 2: Build the Autoencoder (AE)\n# ===============================\ndef build_autoencoder(input_dim, encoding_dim=16):\n    input_layer = Input(shape=(input_dim,))\n    # Encoder\n    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n    # Decoder\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    encoder = Model(inputs=input_layer, outputs=encoded)\n    autoencoder.compile(optimizer='adam', loss='mse')\n    return autoencoder, encoder\n\ninput_dim = X_measure.shape[1]\n\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# 1) Split into train/test (stratified to keep class ratio)\nX_train, X_test, y_train, y_test = train_test_split(\n    X_measure, y, test_size=0.2, random_state=42, stratify=y\n)\n\n# 2) Train AE on NORMAL samples only (y_train == 0)\nX_train_norm = X_train[y_train == 0]\n\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Define EarlyStopping: stop after 5 epochs with no improvement in val_loss\nes = EarlyStopping(\n    monitor='val_loss',       # track validation loss (MSE)\n    patience=5,               # wait 5 epochs for improvement\n    restore_best_weights=True # roll back to the best weights\n)\n\n# Retrain autoencoder with EarlyStopping\nautoencoder, ae_encoder = build_autoencoder(input_dim, encoding_dim=16)\nae_history = autoencoder.fit(\n    X_train_norm, X_train_norm,\n    epochs=100,               # a large max so ES can kick in\n    batch_size=32,\n    validation_split=0.1,\n    callbacks=[es],\n    verbose=1\n)\n\n# 3) Compute reconstruction errors on TRAIN set (for thresholding)\nX_train_pred = autoencoder.predict(X_train_norm)\nmse_train = np.mean(np.square(X_train_norm - X_train_pred), axis=1)\n\n# 4) Choose threshold (e.g. 95th percentile of train‐MSE)\nthreshold = np.percentile(mse_train, 95)\nprint(f\"Reconstruction error threshold: {threshold:.5f}\")\n\n# 5) Compute reconstruction errors on TEST set\nX_test_pred = autoencoder.predict(X_test)\nmse_test = np.mean(np.square(X_test - X_test_pred), axis=1)\n\n# 6) Predict: 1 if error > threshold, else 0\ny_pred = (mse_test > threshold).astype(int)\n\n# 7) Print evaluation metrics\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\nprint(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\nprint(\"\\nAE_Classification Report:\\n\",\n      classification_report(\n          y_test, y_pred,\n          target_names=['Normal', 'Attacked'],\n          digits=4\n      ))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:08:02.017234Z","iopub.execute_input":"2025-05-09T12:08:02.017533Z","iopub.status.idle":"2025-05-09T12:08:53.099075Z","shell.execute_reply.started":"2025-05-09T12:08:02.017512Z","shell.execute_reply":"2025-05-09T12:08:53.09826Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - loss: 1.4271 - val_loss: 0.9692\nEpoch 2/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9435 - val_loss: 0.8641\nEpoch 3/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8789 - val_loss: 0.8438\nEpoch 4/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8840 - val_loss: 0.8370\nEpoch 5/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8217 - val_loss: 0.8340\nEpoch 6/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8223 - val_loss: 0.8324\nEpoch 7/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8340 - val_loss: 0.8316\nEpoch 8/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8168 - val_loss: 0.8310\nEpoch 9/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8563 - val_loss: 0.8307\nEpoch 10/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8148 - val_loss: 0.8305\nEpoch 11/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8204 - val_loss: 0.8303\nEpoch 12/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8386 - val_loss: 0.8301\nEpoch 13/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8584 - val_loss: 0.8300\nEpoch 14/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8458 - val_loss: 0.8298\nEpoch 15/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8716 - val_loss: 0.8297\nEpoch 16/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8334 - val_loss: 0.8296\nEpoch 17/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8183 - val_loss: 0.8294\nEpoch 18/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8646 - val_loss: 0.8293\nEpoch 19/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8244 - val_loss: 0.8292\nEpoch 20/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8088 - val_loss: 0.8291\nEpoch 21/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8364 - val_loss: 0.8289\nEpoch 22/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8576 - val_loss: 0.8288\nEpoch 23/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8346 - val_loss: 0.8287\nEpoch 24/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8569 - val_loss: 0.8286\nEpoch 25/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8329 - val_loss: 0.8286\nEpoch 26/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8576 - val_loss: 0.8286\nEpoch 27/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8191 - val_loss: 0.8285\nEpoch 28/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8226 - val_loss: 0.8284\nEpoch 29/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8539 - val_loss: 0.8284\nEpoch 30/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8421 - val_loss: 0.8284\nEpoch 31/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8502 - val_loss: 0.8283\nEpoch 32/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8127 - val_loss: 0.8283\nEpoch 33/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8160 - val_loss: 0.8283\nEpoch 34/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8342 - val_loss: 0.8282\nEpoch 35/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8296 - val_loss: 0.8282\nEpoch 36/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8204 - val_loss: 0.8282\nEpoch 37/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8368 - val_loss: 0.8282\nEpoch 38/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8105 - val_loss: 0.8282\nEpoch 39/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8627 - val_loss: 0.8282\nEpoch 40/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8332 - val_loss: 0.8282\nEpoch 41/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8092 - val_loss: 0.8281\nEpoch 42/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7968 - val_loss: 0.8281\nEpoch 43/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8395 - val_loss: 0.8281\nEpoch 44/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8269 - val_loss: 0.8281\nEpoch 45/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8334 - val_loss: 0.8281\nEpoch 46/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8394 - val_loss: 0.8281\nEpoch 47/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8125 - val_loss: 0.8281\nEpoch 48/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8186 - val_loss: 0.8281\nEpoch 49/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8073 - val_loss: 0.8281\nEpoch 50/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8204 - val_loss: 0.8281\nEpoch 51/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.7963 - val_loss: 0.8280\nEpoch 52/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8297 - val_loss: 0.8280\nEpoch 53/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8111 - val_loss: 0.8280\nEpoch 54/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8102 - val_loss: 0.8280\nEpoch 55/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8236 - val_loss: 0.8280\nEpoch 56/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8217 - val_loss: 0.8279\nEpoch 57/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8398 - val_loss: 0.8279\nEpoch 58/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8358 - val_loss: 0.8279\nEpoch 59/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8170 - val_loss: 0.8279\nEpoch 60/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8001 - val_loss: 0.8279\nEpoch 61/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7856 - val_loss: 0.8279\nEpoch 62/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8316 - val_loss: 0.8279\nEpoch 63/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8244 - val_loss: 0.8279\nEpoch 64/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8493 - val_loss: 0.8279\nEpoch 65/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8645 - val_loss: 0.8279\nEpoch 66/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8273 - val_loss: 0.8279\nEpoch 67/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8192 - val_loss: 0.8279\nEpoch 68/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8256 - val_loss: 0.8278\nEpoch 69/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8058 - val_loss: 0.8278\nEpoch 70/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8112 - val_loss: 0.8278\nEpoch 71/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8431 - val_loss: 0.8278\nEpoch 72/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8185 - val_loss: 0.8278\nEpoch 73/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8173 - val_loss: 0.8278\nEpoch 74/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8070 - val_loss: 0.8278\nEpoch 75/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8326 - val_loss: 0.8278\nEpoch 76/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8295 - val_loss: 0.8278\nEpoch 77/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8160 - val_loss: 0.8278\nEpoch 78/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8265 - val_loss: 0.8278\nEpoch 79/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8353 - val_loss: 0.8278\nEpoch 80/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8093 - val_loss: 0.8278\nEpoch 81/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8467 - val_loss: 0.8278\nEpoch 82/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8702 - val_loss: 0.8278\nEpoch 83/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8307 - val_loss: 0.8278\nEpoch 84/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8380 - val_loss: 0.8278\nEpoch 85/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8321 - val_loss: 0.8278\nEpoch 86/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8263 - val_loss: 0.8278\nEpoch 87/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.7986 - val_loss: 0.8278\nEpoch 88/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8201 - val_loss: 0.8278\nEpoch 89/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8229 - val_loss: 0.8278\nEpoch 90/100\n\u001b[1m382/382\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8534 - val_loss: 0.8278\n\u001b[1m424/424\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\nReconstruction error threshold: 6.57545\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\nAccuracy: 0.6796858855135358\n\nConfusion Matrix:\n [[3232  155]\n [1395   57]]\n\nAE_Classification Report:\n               precision    recall  f1-score   support\n\n      Normal     0.6985    0.9542    0.8066      3387\n    Attacked     0.2689    0.0393    0.0685      1452\n\n    accuracy                         0.6797      4839\n   macro avg     0.4837    0.4967    0.4375      4839\nweighted avg     0.5696    0.6797    0.5851      4839\n\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ===============================\n# Step 3: Build the Graph Neural Network (GNN)\n# ===============================\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom spektral.layers import GCNConv\nimport numpy as np\n\nclass CustomGCNConv(GCNConv):\n    def compute_output_shape(self, input_shapes):\n        features_shape = input_shapes[0]\n        return (features_shape[0], features_shape[1], self.channels)\n        \n    def call(self, inputs, mask=None):\n        features, adjacency = inputs\n        output = super().call(inputs)\n        return output\n\ndef build_gnn(node_feature_dim, n_hidden=64, n_classes=16, n_nodes=None):\n    if n_nodes is None:\n        raise ValueError(\"n_nodes must be specified.\")\n        \n    X_in = Input(shape=(n_nodes, node_feature_dim))\n    A_in = Input(shape=(n_nodes, n_nodes), sparse=False)\n    \n    gc1 = CustomGCNConv(\n        n_hidden,\n        activation='relu',\n        kernel_initializer='glorot_uniform',\n        use_bias=True\n    )\n    gc1_out = gc1([X_in, A_in])\n    \n    gc2 = CustomGCNConv(\n        n_classes,\n        activation='relu',\n        kernel_initializer='glorot_uniform',\n        use_bias=True\n    )\n    gc2_out = gc2([gc1_out, A_in])\n    \n    model = Model(inputs=[X_in, A_in], outputs=gc2_out)\n    model.compile(\n        optimizer=Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae']\n    )\n    return model\n\ndef prepare_graph_data(node_features, adjacency, batch_size=1):\n    \"\"\"\n    Prepare graph data for training by adding batch dimension and normalizing adjacency matrix.\n    \"\"\"\n    from spektral.utils import normalized_adjacency\n    \n    # Ensure inputs are float32\n    node_features = node_features.astype(np.float32)\n    adjacency = adjacency.astype(np.float32)\n    \n    # Normalize adjacency matrix\n    adj_normalized = normalized_adjacency(adjacency)\n    \n    # Add batch dimension if not already present\n    if len(node_features.shape) == 2:\n        node_features_batch = np.expand_dims(node_features, axis=0)\n    else:\n        node_features_batch = node_features\n        \n    if len(adj_normalized.shape) == 2:\n        adj_normalized_batch = np.expand_dims(adj_normalized, axis=0)\n    else:\n        adj_normalized_batch = adj_normalized\n    \n    return node_features_batch, adj_normalized_batch\n\ndef example_usage(n_nodes=34, feature_dim=16):\n    # Create dummy data\n    node_features = np.random.randn(n_nodes, feature_dim).astype(np.float32)\n    adjacency = np.random.randint(0, 2, size=(n_nodes, n_nodes)).astype(np.float32)\n    # Make adjacency matrix symmetric\n    adjacency = np.maximum(adjacency, adjacency.T)\n    np.fill_diagonal(adjacency, 1)  # Add self-loops\n    \n    # Build model\n    model = build_gnn(\n        node_feature_dim=feature_dim,\n        n_hidden=32,\n        n_classes=feature_dim,\n        n_nodes=n_nodes\n    )\n    \n    # Prepare data\n    features_batch, adj_batch = prepare_graph_data(node_features, adjacency)\n    \n    # Create dummy targets (autoencoder-like reconstruction)\n    targets_batch = np.expand_dims(node_features, axis=0)\n    \n    # Train model\n    history = model.fit(\n        [features_batch, adj_batch],\n        targets_batch,\n        epochs=50,\n        batch_size=1,\n        verbose=1\n    )\n    \n    return model, history\n\n# Example usage\nn_nodes = 34\nfeature_dim = 16\ngnn_model, history = example_usage(n_nodes=n_nodes, feature_dim=feature_dim)\n\nimport matplotlib.pyplot as plt\n\ndef plot_performance(history):\n    plt.figure(figsize=(12, 4))\n    \n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(history.history['loss'], label='Training Loss')\n    plt.title('Training Loss (MSE)')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Plot MAE\n    plt.subplot(1, 2, 2)\n    plt.plot(history.history['mae'], label='Training MAE', color='orange')\n    plt.title('Training MAE')\n    plt.xlabel('Epoch')\n    plt.ylabel('MAE')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n# Plot the training metrics\nplot_performance(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:09:10.47511Z","iopub.execute_input":"2025-05-09T12:09:10.475407Z","iopub.status.idle":"2025-05-09T12:09:13.524442Z","shell.execute_reply.started":"2025-05-09T12:09:10.475386Z","shell.execute_reply":"2025-05-09T12:09:13.523556Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0120 - mae: 0.7990\nEpoch 2/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0112 - mae: 0.7986\nEpoch 3/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0104 - mae: 0.7983\nEpoch 4/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0097 - mae: 0.7979\nEpoch 5/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0089 - mae: 0.7976\nEpoch 6/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0082 - mae: 0.7973\nEpoch 7/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0075 - mae: 0.7971\nEpoch 8/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0072 - mae: 0.7969\nEpoch 9/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0070 - mae: 0.7967\nEpoch 10/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0067 - mae: 0.7966\nEpoch 11/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0065 - mae: 0.7965\nEpoch 12/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0063 - mae: 0.7963\nEpoch 13/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0061 - mae: 0.7962\nEpoch 14/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0059 - mae: 0.7960\nEpoch 15/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0057 - mae: 0.7959\nEpoch 16/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0055 - mae: 0.7958\nEpoch 17/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0053 - mae: 0.7956\nEpoch 18/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0051 - mae: 0.7955\nEpoch 19/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0049 - mae: 0.7954\nEpoch 20/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0047 - mae: 0.7952\nEpoch 21/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0045 - mae: 0.7951\nEpoch 22/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0044 - mae: 0.7950\nEpoch 23/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0042 - mae: 0.7948\nEpoch 24/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0041 - mae: 0.7947\nEpoch 25/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0039 - mae: 0.7946\nEpoch 26/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0038 - mae: 0.7945\nEpoch 27/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0036 - mae: 0.7944\nEpoch 28/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0035 - mae: 0.7942\nEpoch 29/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0034 - mae: 0.7941\nEpoch 30/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0033 - mae: 0.7940\nEpoch 31/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0031 - mae: 0.7939\nEpoch 32/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0030 - mae: 0.7938\nEpoch 33/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0029 - mae: 0.7937\nEpoch 34/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0028 - mae: 0.7936\nEpoch 35/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0028 - mae: 0.7935\nEpoch 36/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0027 - mae: 0.7935\nEpoch 37/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0027 - mae: 0.7934\nEpoch 38/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0026 - mae: 0.7933\nEpoch 39/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0026 - mae: 0.7933\nEpoch 40/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0026 - mae: 0.7932\nEpoch 41/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0025 - mae: 0.7931\nEpoch 42/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0025 - mae: 0.7931\nEpoch 43/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0025 - mae: 0.7930\nEpoch 44/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0024 - mae: 0.7930\nEpoch 45/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0024 - mae: 0.7930\nEpoch 46/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0024 - mae: 0.7929\nEpoch 47/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0023 - mae: 0.7929\nEpoch 48/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0023 - mae: 0.7929\nEpoch 49/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.0022 - mae: 0.7929\nEpoch 50/50\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.0022 - mae: 0.7929\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1200x400 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ===============================\n# Step 4: Build the RNN with Attention Mechanism (RNN+AM)\n# ===============================\n# Custom attention layer (suitable for many-to-one tasks)\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n        super(Attention, self).build(input_shape)\n    def call(self, x):\n        # Compute scores\n        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n        a = tf.keras.backend.softmax(e, axis=1)\n        output = x * a\n        return tf.keras.backend.sum(output, axis=1)\n\ndef build_rnn_attention(input_shape, lstm_units=64):\n    input_layer = Input(shape=input_shape)\n    lstm_out = LSTM(lstm_units, return_sequences=True)(input_layer)\n    attn_out = Attention()(lstm_out)\n    dense_out = Dense(128, activation='relu')(attn_out)\n    output = Dense(1, activation='sigmoid')(dense_out)\n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# For demonstration, reshape the measurement data into sequences.\n# (This example creates sequences of length 5; adjust based on your data.)\ntimesteps = 5\nif X_measure.shape[0] % timesteps != 0:\n    # trim data to form complete sequences\n    trim = X_measure.shape[0] - (X_measure.shape[0] % timesteps)\n    X_measure_seq = X_measure[:trim]\n    y_seq = y[:trim]\nelse:\n    X_measure_seq = X_measure\n    y_seq = y\n\nX_rnn = X_measure_seq.reshape(-1, timesteps, input_dim)\ny_rnn = y_seq.reshape(-1, timesteps)[:, -1]  # use the last label in each sequence as target\n\nrnn_model = build_rnn_attention(input_shape=(timesteps, input_dim), lstm_units=32)\nrnn_model.fit(X_rnn, y_rnn, epochs=10, batch_size=32, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:10:02.326927Z","iopub.execute_input":"2025-05-09T12:10:02.327246Z","iopub.status.idle":"2025-05-09T12:10:12.427275Z","shell.execute_reply.started":"2025-05-09T12:10:02.327223Z","shell.execute_reply":"2025-05-09T12:10:12.426596Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.6908 - loss: 0.6432 - val_accuracy: 0.6736 - val_loss: 0.6317\nEpoch 2/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6957 - loss: 0.6158 - val_accuracy: 0.6736 - val_loss: 0.6322\nEpoch 3/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7018 - loss: 0.6095 - val_accuracy: 0.6736 - val_loss: 0.6325\nEpoch 4/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6971 - loss: 0.6119 - val_accuracy: 0.6736 - val_loss: 0.6328\nEpoch 5/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6903 - loss: 0.6186 - val_accuracy: 0.6736 - val_loss: 0.6342\nEpoch 6/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6917 - loss: 0.6175 - val_accuracy: 0.6736 - val_loss: 0.6350\nEpoch 7/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6890 - loss: 0.6188 - val_accuracy: 0.6736 - val_loss: 0.6333\nEpoch 8/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7038 - loss: 0.6091 - val_accuracy: 0.6736 - val_loss: 0.6353\nEpoch 9/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6869 - loss: 0.6199 - val_accuracy: 0.6736 - val_loss: 0.6357\nEpoch 10/10\n\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6798 - loss: 0.6269 - val_accuracy: 0.6736 - val_loss: 0.6380\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a81341743d0>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"# ===============================\n# Step 5: Build the Deep Belief Network (DBN) Module\n# ===============================\n# Revised: Use the Functional API for better flexibility and to define inputs properly.\ndef build_dbn(input_dim, hidden_dims=[64, 32], output_dim=1):\n    inputs = Input(shape=(input_dim,))\n    x = Dense(hidden_dims[0], activation='relu')(inputs)\n    for units in hidden_dims[1:]:\n        x = Dense(units, activation='relu')(x)\n    outputs = Dense(output_dim, activation='sigmoid')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ndbn_model = build_dbn(input_dim=input_dim, hidden_dims=[64, 32], output_dim=1)\ndbn_model.fit(X_measure, y, epochs=10, batch_size=32, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:10:20.227902Z","iopub.execute_input":"2025-05-09T12:10:20.228211Z","iopub.status.idle":"2025-05-09T12:10:32.678502Z","shell.execute_reply.started":"2025-05-09T12:10:20.228186Z","shell.execute_reply":"2025-05-09T12:10:32.677825Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6993 - loss: 0.6223 - val_accuracy: 0.6893 - val_loss: 0.6191\nEpoch 2/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.6994 - loss: 0.6117 - val_accuracy: 0.6893 - val_loss: 0.6193\nEpoch 3/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7022 - loss: 0.6089 - val_accuracy: 0.6893 - val_loss: 0.6198\nEpoch 4/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7042 - loss: 0.6065 - val_accuracy: 0.6893 - val_loss: 0.6221\nEpoch 5/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7031 - loss: 0.6069 - val_accuracy: 0.6893 - val_loss: 0.6210\nEpoch 6/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7044 - loss: 0.6046 - val_accuracy: 0.6897 - val_loss: 0.6187\nEpoch 7/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7075 - loss: 0.6022 - val_accuracy: 0.6901 - val_loss: 0.6214\nEpoch 8/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7030 - loss: 0.6063 - val_accuracy: 0.6909 - val_loss: 0.6182\nEpoch 9/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7087 - loss: 0.6001 - val_accuracy: 0.6913 - val_loss: 0.6177\nEpoch 10/10\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7123 - loss: 0.5949 - val_accuracy: 0.6926 - val_loss: 0.6155\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a80e7394640>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ===============================\n# Step 6: Build the Generative Adversarial Network (GAN)\n# ===============================\n# GAN components: generator and discriminator.\nlatent_dim = 16  # dimension of the latent noise vector\noutput_dim = input_dim  # generating synthetic measurement feature vectors\n\ndef build_generator(latent_dim, output_dim):\n    model = Sequential()\n    model.add(Dense(128, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(output_dim, activation='tanh'))\n    return model\n\ndef build_discriminator(input_dim):\n    model = Sequential()\n    model.add(Dense(256, input_dim=input_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(128))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ngenerator = build_generator(latent_dim, output_dim)\ndiscriminator = build_discriminator(output_dim)\n\n# Build combined GAN model\ndef build_gan(generator, discriminator, latent_dim):\n    discriminator.trainable = False  # freeze discriminator during generator training\n    gan_input = Input(shape=(latent_dim,))\n    generated_sample = generator(gan_input)\n    gan_output = discriminator(generated_sample)\n    gan_model = Model(gan_input, gan_output)\n    gan_model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n    return gan_model\n\ngan_model = build_gan(generator, discriminator, latent_dim)\n\n# Train the GAN (simplified training loop)\nepochs = 50\nbatch_size = 32\nfor epoch in range(epochs):\n    # --- Train Discriminator ---\n    # Select a random batch of real samples\n    idx = np.random.randint(0, X_measure.shape[0], batch_size)\n    real_samples = X_measure[idx]\n    # Generate fake samples\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    fake_samples = generator.predict(noise)\n    # Labels for real and fake data\n    d_loss_real = discriminator.train_on_batch(real_samples, np.ones((batch_size, 1)))\n    d_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((batch_size, 1)))\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n    \n    # --- Train Generator ---\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    g_loss = gan_model.train_on_batch(noise, np.ones((batch_size, 1)))\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}: [D loss: {d_loss[0]}, acc.: {d_loss[1]}] [G loss: {g_loss}]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:10:53.547151Z","iopub.execute_input":"2025-05-09T12:10:53.547446Z","iopub.status.idle":"2025-05-09T12:11:11.437563Z","shell.execute_reply.started":"2025-05-09T12:10:53.547423Z","shell.execute_reply":"2025-05-09T12:11:11.43687Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 221ms/step\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py:75: UserWarning: The model does not have any trainable weights.\n  warnings.warn(\"The model does not have any trainable weights.\")\n","output_type":"stream"},{"name":"stdout","text":"Epoch 0: [D loss: 0.716407060623169, acc.: 0.1953125] [G loss: [array(0.7063776, dtype=float32), array(0.7063776, dtype=float32), array(0.359375, dtype=float32)]]\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ===============================\n# Step 7: Build the Hybrid Model (Model Integration)\n# ===============================\n# --- Step A: Build Feature Extractors for RNN and DBN Branches ---\n\n# For the RNN branch, extract the 128-dimensional feature vector before the final classification.\ndef build_rnn_feature_extractor(rnn_model):\n    # Assumes rnn_model layers: [Input, LSTM, Attention, Dense(128, activation='relu'), Dense(1, activation='sigmoid')]\n    feature_extractor = Model(inputs=rnn_model.input, outputs=rnn_model.layers[-2].output)\n    return feature_extractor\n\nrnn_feature_extractor = build_rnn_feature_extractor(rnn_model)\n\n# For the DBN branch, extract the features from the penultimate layer.\ndef build_dbn_feature_extractor(dbn_model):\n    feature_extractor = Model(inputs=dbn_model.input, outputs=dbn_model.layers[-2].output)\n    return feature_extractor\n\ndbn_feature_extractor = build_dbn_feature_extractor(dbn_model)\n\n# --- Step B: Build the Enhanced Hybrid Model ---\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization, RepeatVector\n\ndef build_enhanced_hybrid(ae_encoder, rnn_feature_extractor, dbn_feature_extractor, input_dim, timesteps=5):\n    main_input = Input(shape=(input_dim,))\n    \n    # AE Branch: Extract encoded features (e.g., dimension 16)\n    ae_features = ae_encoder(main_input)  # shape: (None, 16)\n    \n    # RNN Branch: Create a sequence input and extract features (e.g., dimension 128)\n    rnn_input = RepeatVector(timesteps)(main_input)  # shape: (None, timesteps, input_dim)\n    rnn_features = rnn_feature_extractor(rnn_input)   # shape: (None, 128)\n    \n    # DBN Branch: Extract features (e.g., dimension 32 if hidden_dims=[64,32])\n    dbn_features = dbn_feature_extractor(main_input)    # shape: (None, 32)\n    \n    # Concatenate features from all branches\n    combined = Concatenate()([ae_features, rnn_features, dbn_features])\n    \n    # Fully connected layers for final discrimination\n    x = Dense(256, activation='relu')(combined)\n    x = Dropout(0.3)(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    output = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=main_input, outputs=output)\n    model.compile(\n        optimizer=Adam(learning_rate=0.0005),\n        loss='binary_crossentropy',\n        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n    )\n    return model\n\n# Build the hybrid model using the extracted feature branches\nhybrid_model = build_enhanced_hybrid(ae_encoder, rnn_feature_extractor, dbn_feature_extractor, input_dim, timesteps=5)\n\n# --- Step C: Train and Evaluate the Hybrid Model ---\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n\n# Stratified split to maintain class balance\nX_train, X_test, y_train, y_test = train_test_split(X_measure, y, test_size=0.2, \n                                                    stratify=y, random_state=42)\n\n# Dynamic learning rate scheduler\ndef lr_scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return float(lr * tf.math.exp(-0.1))\n\ncallbacks = [\n    EarlyStopping(patience=15, restore_best_weights=True),\n    LearningRateScheduler(lr_scheduler)\n]\n\n# Adjust class weights for imbalance\nclass_weights = {0: 1.0, 1: (len(y_train) - np.sum(y_train)) / np.sum(y_train)}\n\nhistory = hybrid_model.fit(\n    X_train, y_train,\n    epochs=100,\n    batch_size=256,\n    validation_split=0.2,\n    class_weight=class_weights,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# Evaluation: use default threshold 0.5 for classification\ny_pred_prob = hybrid_model.predict(X_test)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Normal', 'FDIA']))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:11:39.697992Z","iopub.execute_input":"2025-05-09T12:11:39.6983Z","iopub.status.idle":"2025-05-09T12:12:44.091585Z","shell.execute_reply.started":"2025-05-09T12:11:39.698277Z","shell.execute_reply":"2025-05-09T12:12:44.09081Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 17ms/step - accuracy: 0.4952 - loss: 1.0718 - precision: 0.3052 - recall: 0.5295 - val_accuracy: 0.4640 - val_loss: 0.6972 - val_precision: 0.3192 - val_recall: 0.6387 - learning_rate: 5.0000e-04\nEpoch 2/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5303 - loss: 1.0049 - precision: 0.3154 - recall: 0.5007 - val_accuracy: 0.6027 - val_loss: 0.6807 - val_precision: 0.3468 - val_recall: 0.3140 - learning_rate: 5.0000e-04\nEpoch 3/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5415 - loss: 0.9917 - precision: 0.3158 - recall: 0.4734 - val_accuracy: 0.5688 - val_loss: 0.6825 - val_precision: 0.3375 - val_recall: 0.4012 - learning_rate: 5.0000e-04\nEpoch 4/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5272 - loss: 0.9798 - precision: 0.3152 - recall: 0.5141 - val_accuracy: 0.5236 - val_loss: 0.6835 - val_precision: 0.3445 - val_recall: 0.5889 - learning_rate: 5.0000e-04\nEpoch 5/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5395 - loss: 0.9718 - precision: 0.3266 - recall: 0.5228 - val_accuracy: 0.4981 - val_loss: 0.6863 - val_precision: 0.3400 - val_recall: 0.6520 - learning_rate: 5.0000e-04\nEpoch 6/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.5471 - loss: 0.9667 - precision: 0.3371 - recall: 0.5121 - val_accuracy: 0.6200 - val_loss: 0.6684 - val_precision: 0.3772 - val_recall: 0.3405 - learning_rate: 5.0000e-04\nEpoch 7/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5492 - loss: 0.9669 - precision: 0.3284 - recall: 0.4807 - val_accuracy: 0.6311 - val_loss: 0.6648 - val_precision: 0.4014 - val_recall: 0.3787 - learning_rate: 5.0000e-04\nEpoch 8/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5593 - loss: 0.9514 - precision: 0.3306 - recall: 0.4926 - val_accuracy: 0.5686 - val_loss: 0.6860 - val_precision: 0.3562 - val_recall: 0.4792 - learning_rate: 5.0000e-04\nEpoch 9/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5632 - loss: 0.9464 - precision: 0.3339 - recall: 0.4991 - val_accuracy: 0.4681 - val_loss: 0.6947 - val_precision: 0.3420 - val_recall: 0.7683 - learning_rate: 5.0000e-04\nEpoch 10/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5279 - loss: 0.9590 - precision: 0.3237 - recall: 0.5341 - val_accuracy: 0.5554 - val_loss: 0.6753 - val_precision: 0.3688 - val_recall: 0.6038 - learning_rate: 5.0000e-04\nEpoch 11/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5404 - loss: 0.9485 - precision: 0.3386 - recall: 0.5505 - val_accuracy: 0.5180 - val_loss: 0.6691 - val_precision: 0.3576 - val_recall: 0.6902 - learning_rate: 4.5242e-04\nEpoch 12/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5571 - loss: 0.9445 - precision: 0.3505 - recall: 0.5594 - val_accuracy: 0.5435 - val_loss: 0.6838 - val_precision: 0.3708 - val_recall: 0.6711 - learning_rate: 4.0937e-04\nEpoch 13/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5605 - loss: 0.9345 - precision: 0.3386 - recall: 0.5241 - val_accuracy: 0.5360 - val_loss: 0.6644 - val_precision: 0.3640 - val_recall: 0.6578 - learning_rate: 3.7041e-04\nEpoch 14/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5728 - loss: 0.9402 - precision: 0.3429 - recall: 0.4904 - val_accuracy: 0.4691 - val_loss: 0.6831 - val_precision: 0.3478 - val_recall: 0.8073 - learning_rate: 3.3516e-04\nEpoch 15/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5503 - loss: 0.9371 - precision: 0.3443 - recall: 0.5630 - val_accuracy: 0.5340 - val_loss: 0.6664 - val_precision: 0.3611 - val_recall: 0.6478 - learning_rate: 3.0327e-04\nEpoch 16/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5862 - loss: 0.9343 - precision: 0.3647 - recall: 0.5087 - val_accuracy: 0.4872 - val_loss: 0.6764 - val_precision: 0.3552 - val_recall: 0.7957 - learning_rate: 2.7441e-04\nEpoch 17/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5564 - loss: 0.9254 - precision: 0.3580 - recall: 0.6078 - val_accuracy: 0.4627 - val_loss: 0.6711 - val_precision: 0.3407 - val_recall: 0.7782 - learning_rate: 2.4829e-04\nEpoch 18/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5778 - loss: 0.9227 - precision: 0.3563 - recall: 0.5367 - val_accuracy: 0.5342 - val_loss: 0.6706 - val_precision: 0.3748 - val_recall: 0.7450 - learning_rate: 2.2466e-04\nEpoch 19/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5671 - loss: 0.9251 - precision: 0.3539 - recall: 0.5598 - val_accuracy: 0.5735 - val_loss: 0.6485 - val_precision: 0.3847 - val_recall: 0.6196 - learning_rate: 2.0328e-04\nEpoch 20/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5633 - loss: 0.9197 - precision: 0.3601 - recall: 0.5931 - val_accuracy: 0.5825 - val_loss: 0.6433 - val_precision: 0.3897 - val_recall: 0.6047 - learning_rate: 1.8394e-04\nEpoch 21/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5977 - loss: 0.9132 - precision: 0.3862 - recall: 0.5778 - val_accuracy: 0.6290 - val_loss: 0.6331 - val_precision: 0.4218 - val_recall: 0.5199 - learning_rate: 1.6644e-04\nEpoch 22/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5853 - loss: 0.9140 - precision: 0.3710 - recall: 0.5585 - val_accuracy: 0.6453 - val_loss: 0.6371 - val_precision: 0.4404 - val_recall: 0.5183 - learning_rate: 1.5060e-04\nEpoch 23/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5898 - loss: 0.9074 - precision: 0.3741 - recall: 0.5833 - val_accuracy: 0.6438 - val_loss: 0.6279 - val_precision: 0.4359 - val_recall: 0.4942 - learning_rate: 1.3627e-04\nEpoch 24/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5912 - loss: 0.9092 - precision: 0.3761 - recall: 0.5753 - val_accuracy: 0.6117 - val_loss: 0.6358 - val_precision: 0.4067 - val_recall: 0.5415 - learning_rate: 1.2330e-04\nEpoch 25/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5889 - loss: 0.9107 - precision: 0.3724 - recall: 0.5569 - val_accuracy: 0.6451 - val_loss: 0.6341 - val_precision: 0.4411 - val_recall: 0.5291 - learning_rate: 1.1156e-04\nEpoch 26/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5926 - loss: 0.9086 - precision: 0.3771 - recall: 0.5812 - val_accuracy: 0.6293 - val_loss: 0.6327 - val_precision: 0.4238 - val_recall: 0.5332 - learning_rate: 1.0095e-04\nEpoch 27/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6013 - loss: 0.8973 - precision: 0.3760 - recall: 0.5782 - val_accuracy: 0.5996 - val_loss: 0.6389 - val_precision: 0.4056 - val_recall: 0.6171 - learning_rate: 9.1342e-05\nEpoch 28/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5872 - loss: 0.9011 - precision: 0.3685 - recall: 0.5615 - val_accuracy: 0.6350 - val_loss: 0.6280 - val_precision: 0.4303 - val_recall: 0.5357 - learning_rate: 8.2649e-05\nEpoch 29/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5947 - loss: 0.9031 - precision: 0.3877 - recall: 0.5966 - val_accuracy: 0.6445 - val_loss: 0.6229 - val_precision: 0.4404 - val_recall: 0.5282 - learning_rate: 7.4784e-05\nEpoch 30/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6020 - loss: 0.8990 - precision: 0.3842 - recall: 0.5776 - val_accuracy: 0.6466 - val_loss: 0.6191 - val_precision: 0.4414 - val_recall: 0.5133 - learning_rate: 6.7668e-05\nEpoch 31/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5894 - loss: 0.9083 - precision: 0.3785 - recall: 0.5773 - val_accuracy: 0.6577 - val_loss: 0.6175 - val_precision: 0.4545 - val_recall: 0.5025 - learning_rate: 6.1228e-05\nEpoch 32/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6016 - loss: 0.9003 - precision: 0.3843 - recall: 0.5707 - val_accuracy: 0.6587 - val_loss: 0.6168 - val_precision: 0.4555 - val_recall: 0.4975 - learning_rate: 5.5402e-05\nEpoch 33/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6117 - loss: 0.8959 - precision: 0.3994 - recall: 0.5666 - val_accuracy: 0.6538 - val_loss: 0.6154 - val_precision: 0.4498 - val_recall: 0.5058 - learning_rate: 5.0129e-05\nEpoch 34/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5937 - loss: 0.9009 - precision: 0.3810 - recall: 0.5753 - val_accuracy: 0.6066 - val_loss: 0.6284 - val_precision: 0.4048 - val_recall: 0.5631 - learning_rate: 4.5359e-05\nEpoch 35/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6004 - loss: 0.8953 - precision: 0.3810 - recall: 0.5681 - val_accuracy: 0.6538 - val_loss: 0.6166 - val_precision: 0.4501 - val_recall: 0.5091 - learning_rate: 4.1042e-05\nEpoch 36/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6057 - loss: 0.9006 - precision: 0.3975 - recall: 0.5638 - val_accuracy: 0.6401 - val_loss: 0.6195 - val_precision: 0.4359 - val_recall: 0.5341 - learning_rate: 3.7137e-05\nEpoch 37/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5894 - loss: 0.8947 - precision: 0.3780 - recall: 0.5760 - val_accuracy: 0.6544 - val_loss: 0.6174 - val_precision: 0.4499 - val_recall: 0.5000 - learning_rate: 3.3603e-05\nEpoch 38/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6049 - loss: 0.8948 - precision: 0.3837 - recall: 0.5660 - val_accuracy: 0.6538 - val_loss: 0.6146 - val_precision: 0.4499 - val_recall: 0.5075 - learning_rate: 3.0405e-05\nEpoch 39/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6051 - loss: 0.8861 - precision: 0.3880 - recall: 0.5839 - val_accuracy: 0.6559 - val_loss: 0.6130 - val_precision: 0.4527 - val_recall: 0.5083 - learning_rate: 2.7512e-05\nEpoch 40/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6052 - loss: 0.8883 - precision: 0.3880 - recall: 0.5859 - val_accuracy: 0.6523 - val_loss: 0.6125 - val_precision: 0.4482 - val_recall: 0.5100 - learning_rate: 2.4893e-05\nEpoch 41/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6052 - loss: 0.8936 - precision: 0.3904 - recall: 0.5676 - val_accuracy: 0.6575 - val_loss: 0.6156 - val_precision: 0.4553 - val_recall: 0.5158 - learning_rate: 2.2525e-05\nEpoch 42/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5992 - loss: 0.8866 - precision: 0.3808 - recall: 0.5767 - val_accuracy: 0.6541 - val_loss: 0.6159 - val_precision: 0.4508 - val_recall: 0.5141 - learning_rate: 2.0381e-05\nEpoch 43/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6023 - loss: 0.8911 - precision: 0.3828 - recall: 0.5651 - val_accuracy: 0.6556 - val_loss: 0.6155 - val_precision: 0.4527 - val_recall: 0.5133 - learning_rate: 1.8442e-05\nEpoch 44/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5982 - loss: 0.8941 - precision: 0.3769 - recall: 0.5589 - val_accuracy: 0.6518 - val_loss: 0.6168 - val_precision: 0.4481 - val_recall: 0.5166 - learning_rate: 1.6687e-05\nEpoch 45/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6040 - loss: 0.8955 - precision: 0.3920 - recall: 0.5681 - val_accuracy: 0.6562 - val_loss: 0.6118 - val_precision: 0.4524 - val_recall: 0.5017 - learning_rate: 1.5099e-05\nEpoch 46/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5953 - loss: 0.8952 - precision: 0.3753 - recall: 0.5657 - val_accuracy: 0.6554 - val_loss: 0.6137 - val_precision: 0.4520 - val_recall: 0.5083 - learning_rate: 1.3662e-05\nEpoch 47/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5961 - loss: 0.8915 - precision: 0.3773 - recall: 0.5699 - val_accuracy: 0.6587 - val_loss: 0.6121 - val_precision: 0.4561 - val_recall: 0.5050 - learning_rate: 1.2362e-05\nEpoch 48/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6037 - loss: 0.8894 - precision: 0.3863 - recall: 0.5782 - val_accuracy: 0.6556 - val_loss: 0.6120 - val_precision: 0.4520 - val_recall: 0.5050 - learning_rate: 1.1185e-05\nEpoch 49/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5995 - loss: 0.8904 - precision: 0.3793 - recall: 0.5612 - val_accuracy: 0.6567 - val_loss: 0.6125 - val_precision: 0.4533 - val_recall: 0.5042 - learning_rate: 1.0121e-05\nEpoch 50/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6129 - loss: 0.8868 - precision: 0.3961 - recall: 0.5869 - val_accuracy: 0.6567 - val_loss: 0.6128 - val_precision: 0.4535 - val_recall: 0.5066 - learning_rate: 9.1578e-06\nEpoch 51/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6142 - loss: 0.8894 - precision: 0.3988 - recall: 0.5873 - val_accuracy: 0.6580 - val_loss: 0.6121 - val_precision: 0.4551 - val_recall: 0.5050 - learning_rate: 8.2863e-06\nEpoch 52/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5978 - loss: 0.8846 - precision: 0.3735 - recall: 0.5752 - val_accuracy: 0.6567 - val_loss: 0.6121 - val_precision: 0.4535 - val_recall: 0.5058 - learning_rate: 7.4978e-06\nEpoch 53/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6054 - loss: 0.8872 - precision: 0.3908 - recall: 0.5863 - val_accuracy: 0.6559 - val_loss: 0.6117 - val_precision: 0.4523 - val_recall: 0.5042 - learning_rate: 6.7843e-06\nEpoch 54/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6127 - loss: 0.8923 - precision: 0.3990 - recall: 0.5770 - val_accuracy: 0.6564 - val_loss: 0.6126 - val_precision: 0.4532 - val_recall: 0.5066 - learning_rate: 6.1387e-06\nEpoch 55/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5965 - loss: 0.8933 - precision: 0.3817 - recall: 0.5684 - val_accuracy: 0.6546 - val_loss: 0.6125 - val_precision: 0.4507 - val_recall: 0.5050 - learning_rate: 5.5545e-06\nEpoch 56/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6194 - loss: 0.8798 - precision: 0.4001 - recall: 0.5874 - val_accuracy: 0.6562 - val_loss: 0.6122 - val_precision: 0.4529 - val_recall: 0.5066 - learning_rate: 5.0259e-06\nEpoch 57/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6097 - loss: 0.8840 - precision: 0.3964 - recall: 0.5812 - val_accuracy: 0.6551 - val_loss: 0.6115 - val_precision: 0.4515 - val_recall: 0.5066 - learning_rate: 4.5476e-06\nEpoch 58/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6121 - loss: 0.8844 - precision: 0.3994 - recall: 0.5749 - val_accuracy: 0.6564 - val_loss: 0.6114 - val_precision: 0.4533 - val_recall: 0.5075 - learning_rate: 4.1149e-06\nEpoch 59/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6062 - loss: 0.8873 - precision: 0.3901 - recall: 0.5882 - val_accuracy: 0.6572 - val_loss: 0.6112 - val_precision: 0.4542 - val_recall: 0.5066 - learning_rate: 3.7233e-06\nEpoch 60/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6103 - loss: 0.8816 - precision: 0.3949 - recall: 0.5817 - val_accuracy: 0.6551 - val_loss: 0.6110 - val_precision: 0.4514 - val_recall: 0.5050 - learning_rate: 3.3690e-06\nEpoch 61/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.6128 - loss: 0.8868 - precision: 0.4003 - recall: 0.5823 - val_accuracy: 0.6564 - val_loss: 0.6109 - val_precision: 0.4531 - val_recall: 0.5058 - learning_rate: 3.0484e-06\nEpoch 62/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6101 - loss: 0.8910 - precision: 0.4005 - recall: 0.5735 - val_accuracy: 0.6569 - val_loss: 0.6107 - val_precision: 0.4540 - val_recall: 0.5083 - learning_rate: 2.7583e-06\nEpoch 63/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6059 - loss: 0.8857 - precision: 0.3828 - recall: 0.5603 - val_accuracy: 0.6572 - val_loss: 0.6111 - val_precision: 0.4543 - val_recall: 0.5083 - learning_rate: 2.4958e-06\nEpoch 64/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6086 - loss: 0.8876 - precision: 0.3911 - recall: 0.5734 - val_accuracy: 0.6575 - val_loss: 0.6109 - val_precision: 0.4547 - val_recall: 0.5091 - learning_rate: 2.2583e-06\nEpoch 65/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6103 - loss: 0.8866 - precision: 0.3903 - recall: 0.5738 - val_accuracy: 0.6575 - val_loss: 0.6111 - val_precision: 0.4547 - val_recall: 0.5091 - learning_rate: 2.0434e-06\nEpoch 66/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6159 - loss: 0.8797 - precision: 0.3963 - recall: 0.5803 - val_accuracy: 0.6569 - val_loss: 0.6110 - val_precision: 0.4541 - val_recall: 0.5091 - learning_rate: 1.8489e-06\nEpoch 67/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6135 - loss: 0.8822 - precision: 0.3949 - recall: 0.5900 - val_accuracy: 0.6567 - val_loss: 0.6109 - val_precision: 0.4536 - val_recall: 0.5075 - learning_rate: 1.6730e-06\nEpoch 68/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6039 - loss: 0.8942 - precision: 0.3900 - recall: 0.5547 - val_accuracy: 0.6575 - val_loss: 0.6109 - val_precision: 0.4547 - val_recall: 0.5091 - learning_rate: 1.5138e-06\nEpoch 69/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6135 - loss: 0.8863 - precision: 0.3969 - recall: 0.5670 - val_accuracy: 0.6562 - val_loss: 0.6110 - val_precision: 0.4531 - val_recall: 0.5091 - learning_rate: 1.3697e-06\nEpoch 70/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6088 - loss: 0.8865 - precision: 0.3941 - recall: 0.5806 - val_accuracy: 0.6580 - val_loss: 0.6109 - val_precision: 0.4554 - val_recall: 0.5091 - learning_rate: 1.2394e-06\nEpoch 71/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6150 - loss: 0.8847 - precision: 0.3939 - recall: 0.5675 - val_accuracy: 0.6572 - val_loss: 0.6109 - val_precision: 0.4544 - val_recall: 0.5091 - learning_rate: 1.1214e-06\nEpoch 72/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6091 - loss: 0.8826 - precision: 0.3898 - recall: 0.5584 - val_accuracy: 0.6569 - val_loss: 0.6106 - val_precision: 0.4539 - val_recall: 0.5066 - learning_rate: 1.0147e-06\nEpoch 73/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6108 - loss: 0.8791 - precision: 0.3837 - recall: 0.5745 - val_accuracy: 0.6569 - val_loss: 0.6107 - val_precision: 0.4540 - val_recall: 0.5083 - learning_rate: 9.1815e-07\nEpoch 74/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6152 - loss: 0.8858 - precision: 0.4016 - recall: 0.5837 - val_accuracy: 0.6572 - val_loss: 0.6109 - val_precision: 0.4545 - val_recall: 0.5100 - learning_rate: 8.3078e-07\nEpoch 75/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6086 - loss: 0.8906 - precision: 0.3945 - recall: 0.5585 - val_accuracy: 0.6567 - val_loss: 0.6112 - val_precision: 0.4538 - val_recall: 0.5100 - learning_rate: 7.5172e-07\nEpoch 76/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6037 - loss: 0.8912 - precision: 0.3843 - recall: 0.5672 - val_accuracy: 0.6580 - val_loss: 0.6111 - val_precision: 0.4556 - val_recall: 0.5108 - learning_rate: 6.8018e-07\nEpoch 77/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6028 - loss: 0.8872 - precision: 0.3838 - recall: 0.5782 - val_accuracy: 0.6575 - val_loss: 0.6106 - val_precision: 0.4546 - val_recall: 0.5075 - learning_rate: 6.1545e-07\nEpoch 78/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6063 - loss: 0.8841 - precision: 0.3850 - recall: 0.5697 - val_accuracy: 0.6575 - val_loss: 0.6106 - val_precision: 0.4547 - val_recall: 0.5083 - learning_rate: 5.5689e-07\nEpoch 79/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6123 - loss: 0.8903 - precision: 0.4021 - recall: 0.5707 - val_accuracy: 0.6569 - val_loss: 0.6109 - val_precision: 0.4540 - val_recall: 0.5083 - learning_rate: 5.0389e-07\nEpoch 80/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6021 - loss: 0.8807 - precision: 0.3839 - recall: 0.5853 - val_accuracy: 0.6572 - val_loss: 0.6109 - val_precision: 0.4545 - val_recall: 0.5100 - learning_rate: 4.5594e-07\nEpoch 81/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6120 - loss: 0.8858 - precision: 0.3911 - recall: 0.5766 - val_accuracy: 0.6577 - val_loss: 0.6108 - val_precision: 0.4551 - val_recall: 0.5091 - learning_rate: 4.1255e-07\nEpoch 82/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6069 - loss: 0.8922 - precision: 0.3878 - recall: 0.5548 - val_accuracy: 0.6575 - val_loss: 0.6105 - val_precision: 0.4545 - val_recall: 0.5066 - learning_rate: 3.7329e-07\nEpoch 83/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6102 - loss: 0.8888 - precision: 0.3904 - recall: 0.5611 - val_accuracy: 0.6572 - val_loss: 0.6105 - val_precision: 0.4542 - val_recall: 0.5066 - learning_rate: 3.3777e-07\nEpoch 84/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6088 - loss: 0.8923 - precision: 0.3943 - recall: 0.5812 - val_accuracy: 0.6569 - val_loss: 0.6107 - val_precision: 0.4541 - val_recall: 0.5091 - learning_rate: 3.0563e-07\nEpoch 85/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6094 - loss: 0.8841 - precision: 0.3914 - recall: 0.5809 - val_accuracy: 0.6572 - val_loss: 0.6108 - val_precision: 0.4544 - val_recall: 0.5091 - learning_rate: 2.7654e-07\nEpoch 86/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6093 - loss: 0.8806 - precision: 0.3916 - recall: 0.5732 - val_accuracy: 0.6572 - val_loss: 0.6106 - val_precision: 0.4543 - val_recall: 0.5083 - learning_rate: 2.5022e-07\nEpoch 87/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6003 - loss: 0.8980 - precision: 0.3881 - recall: 0.5652 - val_accuracy: 0.6575 - val_loss: 0.6103 - val_precision: 0.4545 - val_recall: 0.5066 - learning_rate: 2.2641e-07\nEpoch 88/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5972 - loss: 0.8897 - precision: 0.3810 - recall: 0.5749 - val_accuracy: 0.6572 - val_loss: 0.6104 - val_precision: 0.4543 - val_recall: 0.5083 - learning_rate: 2.0487e-07\nEpoch 89/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6097 - loss: 0.8906 - precision: 0.3962 - recall: 0.5766 - val_accuracy: 0.6572 - val_loss: 0.6105 - val_precision: 0.4543 - val_recall: 0.5083 - learning_rate: 1.8537e-07\nEpoch 90/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6033 - loss: 0.8825 - precision: 0.3777 - recall: 0.5626 - val_accuracy: 0.6575 - val_loss: 0.6106 - val_precision: 0.4547 - val_recall: 0.5083 - learning_rate: 1.6773e-07\nEpoch 91/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6037 - loss: 0.8915 - precision: 0.3871 - recall: 0.5778 - val_accuracy: 0.6577 - val_loss: 0.6107 - val_precision: 0.4550 - val_recall: 0.5083 - learning_rate: 1.5177e-07\nEpoch 92/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6117 - loss: 0.8883 - precision: 0.3955 - recall: 0.5795 - val_accuracy: 0.6577 - val_loss: 0.6105 - val_precision: 0.4550 - val_recall: 0.5083 - learning_rate: 1.3733e-07\nEpoch 93/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6053 - loss: 0.8891 - precision: 0.3848 - recall: 0.5744 - val_accuracy: 0.6577 - val_loss: 0.6107 - val_precision: 0.4551 - val_recall: 0.5091 - learning_rate: 1.2426e-07\nEpoch 94/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5997 - loss: 0.8955 - precision: 0.3841 - recall: 0.5694 - val_accuracy: 0.6572 - val_loss: 0.6106 - val_precision: 0.4544 - val_recall: 0.5091 - learning_rate: 1.1243e-07\nEpoch 95/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6091 - loss: 0.8831 - precision: 0.3929 - recall: 0.5756 - val_accuracy: 0.6582 - val_loss: 0.6106 - val_precision: 0.4558 - val_recall: 0.5091 - learning_rate: 1.0173e-07\nEpoch 96/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6136 - loss: 0.8797 - precision: 0.3953 - recall: 0.5835 - val_accuracy: 0.6577 - val_loss: 0.6108 - val_precision: 0.4552 - val_recall: 0.5100 - learning_rate: 9.2052e-08\nEpoch 97/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6120 - loss: 0.8786 - precision: 0.3927 - recall: 0.5773 - val_accuracy: 0.6575 - val_loss: 0.6110 - val_precision: 0.4547 - val_recall: 0.5091 - learning_rate: 8.3293e-08\nEpoch 98/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6072 - loss: 0.8884 - precision: 0.3965 - recall: 0.5686 - val_accuracy: 0.6575 - val_loss: 0.6111 - val_precision: 0.4547 - val_recall: 0.5083 - learning_rate: 7.5366e-08\nEpoch 99/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.5987 - loss: 0.8905 - precision: 0.3760 - recall: 0.5576 - val_accuracy: 0.6577 - val_loss: 0.6106 - val_precision: 0.4550 - val_recall: 0.5083 - learning_rate: 6.8194e-08\nEpoch 100/100\n\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.6105 - loss: 0.8919 - precision: 0.3973 - recall: 0.5714 - val_accuracy: 0.6572 - val_loss: 0.6106 - val_precision: 0.4543 - val_recall: 0.5083 - learning_rate: 6.1705e-08\n\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\nClassification Report:\n               precision    recall  f1-score   support\n\n      Normal       0.77      0.73      0.75      3387\n        FDIA       0.44      0.51      0.47      1452\n\n    accuracy                           0.66      4839\n   macro avg       0.61      0.62      0.61      4839\nweighted avg       0.67      0.66      0.67      4839\n\nConfusion Matrix:\n [[2461  926]\n [ 717  735]]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom kerastuner import HyperModel, RandomSearch\n\n# --- 1. Fixing the Attention Layer ---\n@tf.keras.utils.register_keras_serializable()\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n        super(Attention, self).build(input_shape)\n    def call(self, x):\n        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n        a = tf.keras.backend.softmax(e, axis=1)\n        output = x * a\n        return tf.keras.backend.sum(output, axis=1)\n\n\n# --- 2. Focal Loss Definition (unchanged) ---\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        eps = 1e-8\n        y_pred = K.clip(y_pred, eps, 1. - eps)\n        pt = tf.where(tf.equal(y_true, 1), y_pred, 1-y_pred)\n        loss = -K.sum(alpha * K.pow(1-pt, gamma) * K.log(pt), axis=-1)\n        return loss\n    return focal_loss_fixed\n\n# Optionally, compile your hybrid_model (outside hypermodel) with focal loss:\nhybrid_model.compile(\n    optimizer=Adam(learning_rate=0.0005),\n    loss=focal_loss(gamma=2., alpha=.25),\n    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n)\n\n# --- 3. Create a function to get fresh callbacks ---\ndef get_callbacks():\n    return [\n        EarlyStopping(patience=15, restore_best_weights=True),\n        LearningRateScheduler(lambda epoch, lr: lr if epoch < 10 else float(lr * tf.math.exp(-0.1)))\n    ]\n\n# --- 4. HyperModel for Hyperparameter Tuning ---\nclass HybridHyperModel(HyperModel):\n    def build(self, hp):\n        # Adjust number of units, dropout rates, etc.\n        units = hp.Int('units', min_value=128, max_value=512, step=64)\n        dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n        \n        main_input = Input(shape=(input_dim,))\n        ae_features = ae_encoder(main_input)\n        rnn_input = RepeatVector(timesteps)(main_input)\n        rnn_features = rnn_feature_extractor(rnn_input)\n        dbn_features = dbn_feature_extractor(main_input)\n        combined = Concatenate()([ae_features, rnn_features, dbn_features])\n        \n        x = Dense(units, activation='relu')(combined)\n        x = Dropout(dropout_rate)(x)\n        x = BatchNormalization()(x)\n        x = Dense(units // 2, activation='relu')(x)\n        x = Dropout(dropout_rate / 2)(x)\n        output = Dense(1, activation='sigmoid')(x)\n        model = Model(inputs=main_input, outputs=output)\n        \n        # Optionally, you can compile with focal loss:\n        model.compile(\n            optimizer=Adam(learning_rate=0.0005),\n            loss=focal_loss(gamma=2., alpha=.25),\n            metrics=['accuracy']\n        )\n        return model\n\n# --- 5. Hyperparameter Search ---\nhypermodel = HybridHyperModel()\ntuner = RandomSearch(\n    hypermodel,\n    objective='val_accuracy',\n    max_trials=10,\n    executions_per_trial=2,\n    directory='hybrid_tuning',\n    project_name='fdia_detection'\n)\n\n# Note: Use get_callbacks() to pass new callback instances each time.\ntuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=get_callbacks())\nbest_model = tuner.get_best_models(num_models=1)[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T12:13:37.705648Z","iopub.execute_input":"2025-05-09T12:13:37.706004Z","iopub.status.idle":"2025-05-09T13:21:39.409452Z","shell.execute_reply.started":"2025-05-09T12:13:37.705974Z","shell.execute_reply":"2025-05-09T13:21:39.408465Z"}},"outputs":[{"name":"stdout","text":"Trial 10 Complete [00h 06m 46s]\nval_accuracy: 0.7969517111778259\n\nBest val_accuracy So Far: 0.7969517111778259\nTotal elapsed time: 01h 08m 00s\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 44 variables. \n  saveable.load_own_variables(weights_store.get(inner_path))\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred_prob = best_model.predict(X_test)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n# Print classification metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Normal', 'FDIA']))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n# Compute the confusion matrix using your test labels and predictions\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(6, 5))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Normal', 'FDIA'], rotation=45)\nplt.yticks(tick_marks, ['Normal', 'FDIA'])\n\n# Loop over data dimensions and create text annotations.\nthresh = cm.max() / 2.\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, format(cm[i, j], 'd'),\n                 ha=\"center\", va=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel(\"Actual Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n\n# Compute ROC curve and AUC\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, label=\"ROC curve (AUC = %0.2f)\" % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', label=\"Chance\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\n\n# Compute Precision-Recall curve and average precision score\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\navg_precision = average_precision_score(y_test, y_pred_prob)\n\nplt.subplot(1, 2, 2)\nplt.plot(recall, precision, label=\"PR curve (AP = %0.2f)\" % avg_precision)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.legend(loc=\"lower left\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Loss curves\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Accuracy curves\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming y_pred_prob (predicted probabilities) and y_test (ground truth) are defined\n# Separate predicted probabilities for each class:\npreds_normal = y_pred_prob[y_test == 0]\npreds_fdia   = y_pred_prob[y_test == 1]\n\nplt.figure(figsize=(8, 5))\nplt.hist(preds_normal, bins=20, alpha=0.6, label='Normal', color='green')\nplt.hist(preds_fdia, bins=20, alpha=0.6, label='FDIA', color='red')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Predicted Probability Distribution by Class')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\n\n# Compute calibration curve: fraction of positives vs mean predicted probability\nprob_true, prob_pred = calibration_curve(y_test, y_pred_prob, n_bins=10)\n\nplt.figure(figsize=(8, 5))\nplt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Optionally suppress specific FutureWarnings from Seaborn\nwarnings.filterwarnings(\"ignore\", message=\".*use_inf_as_na.*\", module=\"seaborn\")\nwarnings.filterwarnings(\"ignore\", message=\".*When grouping with a length-1 list-like.*\", module=\"seaborn\")\n\n# Replace infinities with NaN before plotting\ndf_plot = measurement_df.replace([np.inf, -np.inf], np.nan).copy()\ndf_plot['Label'] = np.where(y==1, 'FDIA', 'Normal')\n\n# Plot distributions for each feature\nfeatures_to_plot = ['pRES', 'value', 'p_t']\nplt.figure(figsize=(15, 4))\nfor idx, feature in enumerate(features_to_plot):\n    plt.subplot(1, len(features_to_plot), idx+1)\n    sns.histplot(data=df_plot, x=feature, hue='Label', element='step', stat=\"density\", common_norm=False)\n    plt.title(f\"Distribution of {feature}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the correlation matrix for the measurement features\nfeatures_to_plot = ['pRES', 'value', 'p_t']\ncorr_matrix = measurement_df[features_to_plot].corr()\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap of Measurement Features')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}