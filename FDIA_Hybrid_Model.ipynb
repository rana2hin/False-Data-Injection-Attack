{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10644071,"sourceType":"datasetVersion","datasetId":6590626},{"sourceId":10649988,"sourceType":"datasetVersion","datasetId":6594532}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/rana2hin/fdia-final?scriptVersionId=237994601\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nmerged_data = pd.read_parquet('/kaggle/input/fdia-sample-dataset-simbench/merged_data.parquet')\nlines_sub = pd.read_parquet('/kaggle/input/fdia-sample-dataset-simbench/lines_sub.parquet')\n\nmerged_data.info()\nlines_sub.info()\n\nmerged_data.head()\nlines_sub.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:01:17.015161Z","iopub.execute_input":"2025-05-05T17:01:17.015389Z","iopub.status.idle":"2025-05-05T17:01:18.169684Z","shell.execute_reply.started":"2025-05-05T17:01:17.015368Z","shell.execute_reply":"2025-05-05T17:01:18.167021Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 24192 entries, 0 to 24191\nData columns (total 7 columns):\n #   Column   Non-Null Count  Dtype         \n---  ------   --------------  -----         \n 0   time     24192 non-null  datetime64[us]\n 1   id       24192 non-null  object        \n 2   node     24192 non-null  object        \n 3   profile  24192 non-null  object        \n 4   pRES     24192 non-null  float64       \n 5   value    24192 non-null  float64       \n 6   p_t      24192 non-null  float64       \ndtypes: datetime64[us](1), float64(3), object(3)\nmemory usage: 1.3+ MB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20 entries, 0 to 19\nData columns (total 13 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   id          20 non-null     object \n 1   nodeA       20 non-null     object \n 2   nodeB       20 non-null     object \n 3   type        20 non-null     object \n 4   length      20 non-null     float64\n 5   loadingMax  20 non-null     float64\n 6   subnet      20 non-null     object \n 7   voltLvl     20 non-null     float64\n 8   r           19 non-null     float64\n 9   x           19 non-null     float64\n 10  b           19 non-null     float64\n 11  iMax        19 non-null     float64\n 12  type.y      19 non-null     object \ndtypes: float64(7), object(6)\nmemory usage: 2.2+ KB\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n  has_large_values = (abs_vals > 1e6).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n/usr/local/lib/python3.10/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"             id         nodeA         nodeB           type      length  \\\n0  EHV Line 229  EHV Bus 2578  EHV Bus 2579     LineType_4   48.848000   \n1      dcline 3    EHV Bus 49   EHV Bus 499  dcline 1_type  540.000000   \n2  HV1 Line 127   HV1 Bus 320   HV1 Bus 326   Al/St_265/35    0.997712   \n3  HV1 Line 131   HV1 Bus 321   HV1 Bus 330   Al/St_265/35    3.210640   \n4  HV1 Line 140   HV1 Bus 319   HV1 Bus 339   Al/St_265/35    3.546510   \n\n   loadingMax subnet  voltLvl         r         x        b    iMax type.y  \n0       100.0   EHV1      1.0  0.033333  0.333333  3.22799  1950.0    ohl  \n1       100.0   EHV1      1.0       NaN       NaN      NaN     NaN   None  \n2       100.0    HV1      3.0  0.109500  0.296000  2.82740   680.0    ohl  \n3       100.0    HV1      3.0  0.109500  0.296000  2.82740   680.0    ohl  \n4       100.0    HV1      3.0  0.109500  0.296000  2.82740   680.0    ohl  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>nodeA</th>\n      <th>nodeB</th>\n      <th>type</th>\n      <th>length</th>\n      <th>loadingMax</th>\n      <th>subnet</th>\n      <th>voltLvl</th>\n      <th>r</th>\n      <th>x</th>\n      <th>b</th>\n      <th>iMax</th>\n      <th>type.y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>EHV Line 229</td>\n      <td>EHV Bus 2578</td>\n      <td>EHV Bus 2579</td>\n      <td>LineType_4</td>\n      <td>48.848000</td>\n      <td>100.0</td>\n      <td>EHV1</td>\n      <td>1.0</td>\n      <td>0.033333</td>\n      <td>0.333333</td>\n      <td>3.22799</td>\n      <td>1950.0</td>\n      <td>ohl</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dcline 3</td>\n      <td>EHV Bus 49</td>\n      <td>EHV Bus 499</td>\n      <td>dcline 1_type</td>\n      <td>540.000000</td>\n      <td>100.0</td>\n      <td>EHV1</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>None</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>HV1 Line 127</td>\n      <td>HV1 Bus 320</td>\n      <td>HV1 Bus 326</td>\n      <td>Al/St_265/35</td>\n      <td>0.997712</td>\n      <td>100.0</td>\n      <td>HV1</td>\n      <td>3.0</td>\n      <td>0.109500</td>\n      <td>0.296000</td>\n      <td>2.82740</td>\n      <td>680.0</td>\n      <td>ohl</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>HV1 Line 131</td>\n      <td>HV1 Bus 321</td>\n      <td>HV1 Bus 330</td>\n      <td>Al/St_265/35</td>\n      <td>3.210640</td>\n      <td>100.0</td>\n      <td>HV1</td>\n      <td>3.0</td>\n      <td>0.109500</td>\n      <td>0.296000</td>\n      <td>2.82740</td>\n      <td>680.0</td>\n      <td>ohl</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>HV1 Line 140</td>\n      <td>HV1 Bus 319</td>\n      <td>HV1 Bus 339</td>\n      <td>Al/St_265/35</td>\n      <td>3.546510</td>\n      <td>100.0</td>\n      <td>HV1</td>\n      <td>3.0</td>\n      <td>0.109500</td>\n      <td>0.296000</td>\n      <td>2.82740</td>\n      <td>680.0</td>\n      <td>ohl</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"merged_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:01:18.170391Z","iopub.execute_input":"2025-05-05T17:01:18.170798Z","iopub.status.idle":"2025-05-05T17:01:18.195956Z","shell.execute_reply.started":"2025-05-05T17:01:18.170773Z","shell.execute_reply":"2025-05-05T17:01:18.193937Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                 time            id          node profile  pRES     value  \\\n0 2015-12-31 18:00:00  EHV Sgen 228  EHV Bus 2579     WP2   0.0  0.983439   \n1 2015-12-31 18:00:00  EHV Sgen 230  EHV Bus 2578     WP2   0.0  0.983439   \n2 2015-12-31 18:00:00  HV1 Sgen 108   HV1 Bus 323     WP4  68.8  0.980906   \n3 2015-12-31 18:00:00  HV1 Sgen 114   HV1 Bus 330     WP4  18.0  0.980906   \n4 2015-12-31 18:00:00  HV1 Sgen 122   HV1 Bus 342     WP4  10.0  0.980906   \n\n         p_t  \n0   0.000000  \n1   0.000000  \n2  67.486333  \n3  17.656308  \n4   9.809060  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>id</th>\n      <th>node</th>\n      <th>profile</th>\n      <th>pRES</th>\n      <th>value</th>\n      <th>p_t</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2015-12-31 18:00:00</td>\n      <td>EHV Sgen 228</td>\n      <td>EHV Bus 2579</td>\n      <td>WP2</td>\n      <td>0.0</td>\n      <td>0.983439</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2015-12-31 18:00:00</td>\n      <td>EHV Sgen 230</td>\n      <td>EHV Bus 2578</td>\n      <td>WP2</td>\n      <td>0.0</td>\n      <td>0.983439</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2015-12-31 18:00:00</td>\n      <td>HV1 Sgen 108</td>\n      <td>HV1 Bus 323</td>\n      <td>WP4</td>\n      <td>68.8</td>\n      <td>0.980906</td>\n      <td>67.486333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2015-12-31 18:00:00</td>\n      <td>HV1 Sgen 114</td>\n      <td>HV1 Bus 330</td>\n      <td>WP4</td>\n      <td>18.0</td>\n      <td>0.980906</td>\n      <td>17.656308</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2015-12-31 18:00:00</td>\n      <td>HV1 Sgen 122</td>\n      <td>HV1 Bus 342</td>\n      <td>WP4</td>\n      <td>10.0</td>\n      <td>0.980906</td>\n      <td>9.809060</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!pip install spektral","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:01:18.198001Z","iopub.execute_input":"2025-05-05T17:01:18.198287Z","iopub.status.idle":"2025-05-05T17:01:23.380932Z","shell.execute_reply.started":"2025-05-05T17:01:18.198264Z","shell.execute_reply":"2025-05-05T17:01:23.38011Z"}},"outputs":[{"name":"stdout","text":"Collecting spektral\n  Downloading spektral-1.3.1-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from spektral) (1.4.2)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from spektral) (5.3.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from spektral) (3.4.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.26.4)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from spektral) (2.2.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from spektral) (2.32.3)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from spektral) (1.2.2)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from spektral) (1.13.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from spektral) (4.67.1)\nRequirement already satisfied: tensorflow>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from spektral) (2.17.1)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.20.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow>=2.2.0->spektral) (0.37.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->spektral) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->spektral) (2024.12.14)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2024.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->spektral) (2024.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->spektral) (3.5.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow>=2.2.0->spektral) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.2.0->spektral) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.2.0->spektral) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow>=2.2.0->spektral) (0.13.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.2.0->spektral) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.2.0->spektral) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow>=2.2.0->spektral) (3.1.3)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->spektral) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->spektral) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->spektral) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->spektral) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->spektral) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow>=2.2.0->spektral) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.2.0->spektral) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow>=2.2.0->spektral) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow>=2.2.0->spektral) (0.1.2)\nDownloading spektral-1.3.1-py3-none-any.whl (140 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: spektral\nSuccessfully installed spektral-1.3.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ========================\n# Step 0: Import Libraries\n# ========================\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import (Input, Dense, LSTM, Dropout, Concatenate,\n                                     Reshape, Flatten, BatchNormalization,\n                                     LeakyReLU, Activation)\nfrom tensorflow.keras.optimizers import Adam\n\n# For the GNN, we use Spektral (install via: pip install spektral)\nfrom spektral.layers import GCNConv\n\n# For evaluation metrics\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\n# ===============================\n# Step 1: Data Loading & Preprocessing\n# ===============================\n# Example: load your measurement and topology data (adjust file paths as needed)\nmeasurement_df = pd.read_csv('/kaggle/input/simbench-sample-data/measurements.csv', parse_dates=['time'])\ntopology_df = pd.read_csv('/kaggle/input/simbench-sample-data/topology.csv')\noriginal_df = measurement_df.copy()\n\n# FDIA simulation parameters\nattack_ratio = 0.3  # 30% of data is compromised\nmax_attack_strength = 0.4  # Maximum perturbation (40% deviation)\n\n# Generate attack targets\nn_samples = len(measurement_df)\nattack_indices = np.random.choice(n_samples, size=int(n_samples * attack_ratio), replace=False)\n\n# Simulate FDIA attacks\nfor idx in attack_indices:\n    # Randomly select feature to attack (pRES or value)\n    if np.random.rand() < 0.5:\n        # Attack pRES\n        perturbation = 1 + (np.random.rand() - 0.5) * 2 * max_attack_strength\n        measurement_df.loc[idx, 'pRES'] *= perturbation\n    else:\n        # Attack value\n        perturbation = 1 + (np.random.rand() - 0.5) * 2 * max_attack_strength\n        measurement_df.loc[idx, 'value'] *= perturbation\n    \n    # Recalculate p_t to maintain consistency\n    measurement_df.loc[idx, 'p_t'] = measurement_df.loc[idx, 'pRES'] * measurement_df.loc[idx, 'value']\n\n# Create labels: 1 for attacked samples, 0 for normal\ny = np.zeros(n_samples)\ny[attack_indices] = 1\n\n# Use attacked measurements for model input\nmeasurement_features = ['pRES', 'value', 'p_t']\nX_measure = measurement_df[measurement_features].values\n\n# Normalize using robust scaling (more resistant to outliers)\nfrom sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nX_measure = scaler.fit_transform(X_measure)\n\n# For the GNN, you need a graph structure.\n# For example, create an adjacency matrix from topology_df. (This is highly domain dependent.)\n# Here we assume that each row in topology_df represents an edge between nodeA and nodeB.\n# We first build a mapping from node ID to index:\nnodes = pd.unique(topology_df[['nodeA', 'nodeB']].values.ravel())\nnode_to_index = {node: idx for idx, node in enumerate(nodes)}\nn_nodes = len(nodes)\n\n# Create a simple binary adjacency matrix\nadjacency = np.zeros((n_nodes, n_nodes))\nfor _, row in topology_df.iterrows():\n    i = node_to_index[row['nodeA']]\n    j = node_to_index[row['nodeB']]\n    adjacency[i, j] = 1\n    adjacency[j, i] = 1  # assume undirected graph","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:01:23.382809Z","iopub.execute_input":"2025-05-05T17:01:23.383069Z","iopub.status.idle":"2025-05-05T17:01:39.605194Z","shell.execute_reply.started":"2025-05-05T17:01:23.383045Z","shell.execute_reply":"2025-05-05T17:01:39.604426Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ===============================\n# Step 2: Build the Autoencoder (AE)\n# ===============================\ndef build_autoencoder(input_dim, encoding_dim=16):\n    input_layer = Input(shape=(input_dim,))\n    # Encoder\n    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n    # Decoder\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    encoder = Model(inputs=input_layer, outputs=encoded)\n    autoencoder.compile(optimizer='adam', loss='mse')\n    return autoencoder, encoder\n\ninput_dim = X_measure.shape[1]\n\nautoencoder, ae_encoder = build_autoencoder(input_dim, encoding_dim=16)\n\n# Train AE (use validation_split or separate test set as needed)\nautoencoder.fit(X_measure, X_measure, epochs=20, batch_size=32, validation_split=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:01:39.606076Z","iopub.execute_input":"2025-05-05T17:01:39.606286Z","iopub.status.idle":"2025-05-05T17:02:01.36791Z","shell.execute_reply.started":"2025-05-05T17:01:39.606269Z","shell.execute_reply":"2025-05-05T17:02:01.367161Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - loss: 1.1931 - val_loss: 0.7640\nEpoch 2/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.9016 - val_loss: 0.7369\nEpoch 3/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8913 - val_loss: 0.7323\nEpoch 4/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8882 - val_loss: 0.7309\nEpoch 5/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8724 - val_loss: 0.7303\nEpoch 6/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8742 - val_loss: 0.7301\nEpoch 7/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8315 - val_loss: 0.7300\nEpoch 8/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8428 - val_loss: 0.7298\nEpoch 9/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8780 - val_loss: 0.7297\nEpoch 10/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8715 - val_loss: 0.7297\nEpoch 11/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8960 - val_loss: 0.7296\nEpoch 12/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8849 - val_loss: 0.7295\nEpoch 13/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8362 - val_loss: 0.7294\nEpoch 14/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8683 - val_loss: 0.7292\nEpoch 15/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8250 - val_loss: 0.7289\nEpoch 16/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8532 - val_loss: 0.7286\nEpoch 17/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8401 - val_loss: 0.7284\nEpoch 18/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8623 - val_loss: 0.7283\nEpoch 19/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8389 - val_loss: 0.7282\nEpoch 20/20\n\u001b[1m681/681\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 0.8994 - val_loss: 0.7282\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"<keras.src.callbacks.history.History at 0x7a4a83c02e90>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# ===============================\n# Step 3: Build the Graph Neural Network (GNN)\n# ===============================\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Input\nfrom spektral.layers import GCNConv\nimport numpy as np\n\nclass CustomGCNConv(GCNConv):\n    def compute_output_shape(self, input_shapes):\n        features_shape = input_shapes[0]\n        return (features_shape[0], features_shape[1], self.channels)\n        \n    def call(self, inputs, mask=None):\n        features, adjacency = inputs\n        output = super().call(inputs)\n        return output\n\ndef build_gnn(node_feature_dim, n_hidden=64, n_classes=16, n_nodes=None):\n    if n_nodes is None:\n        raise ValueError(\"n_nodes must be specified.\")\n        \n    X_in = Input(shape=(n_nodes, node_feature_dim))\n    A_in = Input(shape=(n_nodes, n_nodes), sparse=False)\n    \n    gc1 = CustomGCNConv(\n        n_hidden,\n        activation='relu',\n        kernel_initializer='glorot_uniform',\n        use_bias=True\n    )\n    gc1_out = gc1([X_in, A_in])\n    \n    gc2 = CustomGCNConv(\n        n_classes,\n        activation='relu',\n        kernel_initializer='glorot_uniform',\n        use_bias=True\n    )\n    gc2_out = gc2([gc1_out, A_in])\n    \n    model = Model(inputs=[X_in, A_in], outputs=gc2_out)\n    model.compile(\n        optimizer=Adam(learning_rate=0.001),\n        loss='mse',\n        metrics=['mae']\n    )\n    return model\n\ndef prepare_graph_data(node_features, adjacency, batch_size=1):\n    \"\"\"\n    Prepare graph data for training by adding batch dimension and normalizing adjacency matrix.\n    \"\"\"\n    from spektral.utils import normalized_adjacency\n    \n    # Ensure inputs are float32\n    node_features = node_features.astype(np.float32)\n    adjacency = adjacency.astype(np.float32)\n    \n    # Normalize adjacency matrix\n    adj_normalized = normalized_adjacency(adjacency)\n    \n    # Add batch dimension if not already present\n    if len(node_features.shape) == 2:\n        node_features_batch = np.expand_dims(node_features, axis=0)\n    else:\n        node_features_batch = node_features\n        \n    if len(adj_normalized.shape) == 2:\n        adj_normalized_batch = np.expand_dims(adj_normalized, axis=0)\n    else:\n        adj_normalized_batch = adj_normalized\n    \n    return node_features_batch, adj_normalized_batch\n\ndef example_usage(n_nodes=34, feature_dim=16):\n    # Create dummy data\n    node_features = np.random.randn(n_nodes, feature_dim).astype(np.float32)\n    adjacency = np.random.randint(0, 2, size=(n_nodes, n_nodes)).astype(np.float32)\n    # Make adjacency matrix symmetric\n    adjacency = np.maximum(adjacency, adjacency.T)\n    np.fill_diagonal(adjacency, 1)  # Add self-loops\n    \n    # Build model\n    model = build_gnn(\n        node_feature_dim=feature_dim,\n        n_hidden=32,\n        n_classes=feature_dim,\n        n_nodes=n_nodes\n    )\n    \n    # Prepare data\n    features_batch, adj_batch = prepare_graph_data(node_features, adjacency)\n    \n    # Create dummy targets (autoencoder-like reconstruction)\n    targets_batch = np.expand_dims(node_features, axis=0)\n    \n    # Train model\n    history = model.fit(\n        [features_batch, adj_batch],\n        targets_batch,\n        epochs=10,\n        batch_size=1,\n        verbose=1\n    )\n    \n    return model, history\n\n# Example usage\nn_nodes = 34\nfeature_dim = 16\ngnn_model, history = example_usage(n_nodes=n_nodes, feature_dim=feature_dim)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:02:01.368733Z","iopub.execute_input":"2025-05-05T17:02:01.369005Z","iopub.status.idle":"2025-05-05T17:02:03.049957Z","shell.execute_reply.started":"2025-05-05T17:02:01.368983Z","shell.execute_reply":"2025-05-05T17:02:03.049066Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 1.0289 - mae: 0.8084\nEpoch 2/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0283 - mae: 0.8081\nEpoch 3/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0276 - mae: 0.8078\nEpoch 4/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0270 - mae: 0.8074\nEpoch 5/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0264 - mae: 0.8071\nEpoch 6/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0258 - mae: 0.8069\nEpoch 7/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.0253 - mae: 0.8066\nEpoch 8/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0247 - mae: 0.8063\nEpoch 9/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0241 - mae: 0.8060\nEpoch 10/10\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 1.0234 - mae: 0.8057\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ===============================\n# Step 4: Build the RNN with Attention Mechanism (RNN+AM)\n# ===============================\n# Custom attention layer (suitable for many-to-one tasks)\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n        super(Attention, self).build(input_shape)\n    def call(self, x):\n        # Compute scores\n        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n        a = tf.keras.backend.softmax(e, axis=1)\n        output = x * a\n        return tf.keras.backend.sum(output, axis=1)\n\ndef build_rnn_attention(input_shape, lstm_units=64):\n    input_layer = Input(shape=input_shape)\n    lstm_out = LSTM(lstm_units, return_sequences=True)(input_layer)\n    attn_out = Attention()(lstm_out)\n    dense_out = Dense(128, activation='relu')(attn_out)\n    output = Dense(1, activation='sigmoid')(dense_out)\n    model = Model(inputs=input_layer, outputs=output)\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\n# For demonstration, reshape the measurement data into sequences.\n# (This example creates sequences of length 5; adjust based on your data.)\ntimesteps = 5\nif X_measure.shape[0] % timesteps != 0:\n    # trim data to form complete sequences\n    trim = X_measure.shape[0] - (X_measure.shape[0] % timesteps)\n    X_measure_seq = X_measure[:trim]\n    y_seq = y[:trim]\nelse:\n    X_measure_seq = X_measure\n    y_seq = y\n\nX_rnn = X_measure_seq.reshape(-1, timesteps, input_dim)\ny_rnn = y_seq.reshape(-1, timesteps)[:, -1]  # use the last label in each sequence as target\n\nrnn_model = build_rnn_attention(input_shape=(timesteps, input_dim), lstm_units=32)\nrnn_model.fit(X_rnn, y_rnn, epochs=10, batch_size=32, validation_split=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# Step 5: Build the Deep Belief Network (DBN) Module\n# ===============================\n# Revised: Use the Functional API for better flexibility and to define inputs properly.\ndef build_dbn(input_dim, hidden_dims=[64, 32], output_dim=1):\n    inputs = Input(shape=(input_dim,))\n    x = Dense(hidden_dims[0], activation='relu')(inputs)\n    for units in hidden_dims[1:]:\n        x = Dense(units, activation='relu')(x)\n    outputs = Dense(output_dim, activation='sigmoid')(x)\n    model = Model(inputs=inputs, outputs=outputs)\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ndbn_model = build_dbn(input_dim=input_dim, hidden_dims=[64, 32], output_dim=1)\ndbn_model.fit(X_measure, y, epochs=10, batch_size=32, validation_split=0.1)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# Step 6: Build the Generative Adversarial Network (GAN)\n# ===============================\n# GAN components: generator and discriminator.\nlatent_dim = 16  # dimension of the latent noise vector\noutput_dim = input_dim  # generating synthetic measurement feature vectors\n\ndef build_generator(latent_dim, output_dim):\n    model = Sequential()\n    model.add(Dense(128, input_dim=latent_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(256))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(output_dim, activation='tanh'))\n    return model\n\ndef build_discriminator(input_dim):\n    model = Sequential()\n    model.add(Dense(256, input_dim=input_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(128))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy', metrics=['accuracy'])\n    return model\n\ngenerator = build_generator(latent_dim, output_dim)\ndiscriminator = build_discriminator(output_dim)\n\n# Build combined GAN model\ndef build_gan(generator, discriminator, latent_dim):\n    discriminator.trainable = False  # freeze discriminator during generator training\n    gan_input = Input(shape=(latent_dim,))\n    generated_sample = generator(gan_input)\n    gan_output = discriminator(generated_sample)\n    gan_model = Model(gan_input, gan_output)\n    gan_model.compile(optimizer=Adam(0.0002, 0.5), loss='binary_crossentropy')\n    return gan_model\n\ngan_model = build_gan(generator, discriminator, latent_dim)\n\n# Train the GAN (simplified training loop)\nepochs = 1000\nbatch_size = 32\nfor epoch in range(epochs):\n    # --- Train Discriminator ---\n    # Select a random batch of real samples\n    idx = np.random.randint(0, X_measure.shape[0], batch_size)\n    real_samples = X_measure[idx]\n    # Generate fake samples\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    fake_samples = generator.predict(noise)\n    # Labels for real and fake data\n    d_loss_real = discriminator.train_on_batch(real_samples, np.ones((batch_size, 1)))\n    d_loss_fake = discriminator.train_on_batch(fake_samples, np.zeros((batch_size, 1)))\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n    \n    # --- Train Generator ---\n    noise = np.random.normal(0, 1, (batch_size, latent_dim))\n    g_loss = gan_model.train_on_batch(noise, np.ones((batch_size, 1)))\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}: [D loss: {d_loss[0]}, acc.: {d_loss[1]}] [G loss: {g_loss}]\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ===============================\n# Step 7: Build the Hybrid Model (Model Integration)\n# ===============================\n# --- Step A: Build Feature Extractors for RNN and DBN Branches ---\n\n# For the RNN branch, extract the 128-dimensional feature vector before the final classification.\ndef build_rnn_feature_extractor(rnn_model):\n    # Assumes rnn_model layers: [Input, LSTM, Attention, Dense(128, activation='relu'), Dense(1, activation='sigmoid')]\n    feature_extractor = Model(inputs=rnn_model.input, outputs=rnn_model.layers[-2].output)\n    return feature_extractor\n\nrnn_feature_extractor = build_rnn_feature_extractor(rnn_model)\n\n# For the DBN branch, extract the features from the penultimate layer.\ndef build_dbn_feature_extractor(dbn_model):\n    feature_extractor = Model(inputs=dbn_model.input, outputs=dbn_model.layers[-2].output)\n    return feature_extractor\n\ndbn_feature_extractor = build_dbn_feature_extractor(dbn_model)\n\n# --- Step B: Build the Enhanced Hybrid Model ---\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization, RepeatVector\n\ndef build_enhanced_hybrid(ae_encoder, rnn_feature_extractor, dbn_feature_extractor, input_dim, timesteps=5):\n    main_input = Input(shape=(input_dim,))\n    \n    # AE Branch: Extract encoded features (e.g., dimension 16)\n    ae_features = ae_encoder(main_input)  # shape: (None, 16)\n    \n    # RNN Branch: Create a sequence input and extract features (e.g., dimension 128)\n    rnn_input = RepeatVector(timesteps)(main_input)  # shape: (None, timesteps, input_dim)\n    rnn_features = rnn_feature_extractor(rnn_input)   # shape: (None, 128)\n    \n    # DBN Branch: Extract features (e.g., dimension 32 if hidden_dims=[64,32])\n    dbn_features = dbn_feature_extractor(main_input)    # shape: (None, 32)\n    \n    # Concatenate features from all branches\n    combined = Concatenate()([ae_features, rnn_features, dbn_features])\n    \n    # Fully connected layers for final discrimination\n    x = Dense(256, activation='relu')(combined)\n    x = Dropout(0.3)(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.2)(x)\n    output = Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inputs=main_input, outputs=output)\n    model.compile(\n        optimizer=Adam(learning_rate=0.0005),\n        loss='binary_crossentropy',\n        metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n    )\n    return model\n\n# Build the hybrid model using the extracted feature branches\nhybrid_model = build_enhanced_hybrid(ae_encoder, rnn_feature_extractor, dbn_feature_extractor, input_dim, timesteps=5)\n\n# --- Step C: Train and Evaluate the Hybrid Model ---\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n\n# Stratified split to maintain class balance\nX_train, X_test, y_train, y_test = train_test_split(X_measure, y, test_size=0.2, \n                                                    stratify=y, random_state=42)\n\n# Dynamic learning rate scheduler\ndef lr_scheduler(epoch, lr):\n    if epoch < 10:\n        return lr\n    else:\n        return float(lr * tf.math.exp(-0.1))\n\ncallbacks = [\n    EarlyStopping(patience=15, restore_best_weights=True),\n    LearningRateScheduler(lr_scheduler)\n]\n\n# Adjust class weights for imbalance\nclass_weights = {0: 1.0, 1: (len(y_train) - np.sum(y_train)) / np.sum(y_train)}\n\nhistory = hybrid_model.fit(\n    X_train, y_train,\n    epochs=100,\n    batch_size=256,\n    validation_split=0.2,\n    class_weight=class_weights,\n    callbacks=callbacks,\n    verbose=1\n)\n\n# Evaluation: use default threshold 0.5 for classification\ny_pred_prob = hybrid_model.predict(X_test)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Normal', 'FDIA']))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input, Dense, Concatenate, Dropout, BatchNormalization, RepeatVector\nfrom tensorflow.keras.models import Model\nfrom kerastuner import HyperModel, RandomSearch\n\n# --- 1. Fixing the Attention Layer ---\n@tf.keras.utils.register_keras_serializable()\nclass Attention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n    def build(self, input_shape):\n        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n                                 initializer=\"normal\")\n        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n                                 initializer=\"zeros\")\n        super(Attention, self).build(input_shape)\n    def call(self, x):\n        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)\n        a = tf.keras.backend.softmax(e, axis=1)\n        output = x * a\n        return tf.keras.backend.sum(output, axis=1)\n\n\n# --- 2. Focal Loss Definition (unchanged) ---\ndef focal_loss(gamma=2., alpha=.25):\n    def focal_loss_fixed(y_true, y_pred):\n        eps = 1e-8\n        y_pred = K.clip(y_pred, eps, 1. - eps)\n        pt = tf.where(tf.equal(y_true, 1), y_pred, 1-y_pred)\n        loss = -K.sum(alpha * K.pow(1-pt, gamma) * K.log(pt), axis=-1)\n        return loss\n    return focal_loss_fixed\n\n# Optionally, compile your hybrid_model (outside hypermodel) with focal loss:\nhybrid_model.compile(\n    optimizer=Adam(learning_rate=0.0005),\n    loss=focal_loss(gamma=2., alpha=.25),\n    metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()]\n)\n\n# --- 3. Create a function to get fresh callbacks ---\ndef get_callbacks():\n    return [\n        EarlyStopping(patience=15, restore_best_weights=True),\n        LearningRateScheduler(lambda epoch, lr: lr if epoch < 10 else float(lr * tf.math.exp(-0.1)))\n    ]\n\n# --- 4. HyperModel for Hyperparameter Tuning ---\nclass HybridHyperModel(HyperModel):\n    def build(self, hp):\n        # Adjust number of units, dropout rates, etc.\n        units = hp.Int('units', min_value=128, max_value=512, step=64)\n        dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n        \n        main_input = Input(shape=(input_dim,))\n        ae_features = ae_encoder(main_input)\n        rnn_input = RepeatVector(timesteps)(main_input)\n        rnn_features = rnn_feature_extractor(rnn_input)\n        dbn_features = dbn_feature_extractor(main_input)\n        combined = Concatenate()([ae_features, rnn_features, dbn_features])\n        \n        x = Dense(units, activation='relu')(combined)\n        x = Dropout(dropout_rate)(x)\n        x = BatchNormalization()(x)\n        x = Dense(units // 2, activation='relu')(x)\n        x = Dropout(dropout_rate / 2)(x)\n        output = Dense(1, activation='sigmoid')(x)\n        model = Model(inputs=main_input, outputs=output)\n        \n        # Optionally, you can compile with focal loss:\n        model.compile(\n            optimizer=Adam(learning_rate=0.0005),\n            loss=focal_loss(gamma=2., alpha=.25),\n            metrics=['accuracy']\n        )\n        return model\n\n# --- 5. Hyperparameter Search ---\nhypermodel = HybridHyperModel()\ntuner = RandomSearch(\n    hypermodel,\n    objective='val_accuracy',\n    max_trials=10,\n    executions_per_trial=2,\n    directory='hybrid_tuning',\n    project_name='fdia_detection'\n)\n\n# Note: Use get_callbacks() to pass new callback instances each time.\ntuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=get_callbacks())\nbest_model = tuner.get_best_models(num_models=1)[0]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make predictions on the test set\ny_pred_prob = best_model.predict(X_test)\ny_pred = (y_pred_prob > 0.5).astype(int)\n\n# Print classification metrics\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=['Normal', 'FDIA']))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\n\n# Compute the confusion matrix using your test labels and predictions\ncm = confusion_matrix(y_test, y_pred)\n\nplt.figure(figsize=(6, 5))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title(\"Confusion Matrix\")\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['Normal', 'FDIA'], rotation=45)\nplt.yticks(tick_marks, ['Normal', 'FDIA'])\n\n# Loop over data dimensions and create text annotations.\nthresh = cm.max() / 2.\nfor i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n        plt.text(j, i, format(cm[i, j], 'd'),\n                 ha=\"center\", va=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\nplt.ylabel(\"Actual Label\")\nplt.xlabel(\"Predicted Label\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n\n# Compute ROC curve and AUC\nfpr, tpr, _ = roc_curve(y_test, y_pred_prob)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(fpr, tpr, label=\"ROC curve (AUC = %0.2f)\" % roc_auc)\nplt.plot([0, 1], [0, 1], 'k--', label=\"Chance\")\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.title(\"Receiver Operating Characteristic\")\nplt.legend(loc=\"lower right\")\n\n# Compute Precision-Recall curve and average precision score\nprecision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\navg_precision = average_precision_score(y_test, y_pred_prob)\n\nplt.subplot(1, 2, 2)\nplt.plot(recall, precision, label=\"PR curve (AP = %0.2f)\" % avg_precision)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.legend(loc=\"lower left\")\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\n\n# Loss curves\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"loss\"], label=\"Training Loss\")\nplt.plot(history.history[\"val_loss\"], label=\"Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training vs. Validation Loss\")\nplt.legend()\n\n# Accuracy curves\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"accuracy\"], label=\"Training Accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Training vs. Validation Accuracy\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n# Assuming y_pred_prob (predicted probabilities) and y_test (ground truth) are defined\n# Separate predicted probabilities for each class:\npreds_normal = y_pred_prob[y_test == 0]\npreds_fdia   = y_pred_prob[y_test == 1]\n\nplt.figure(figsize=(8, 5))\nplt.hist(preds_normal, bins=20, alpha=0.6, label='Normal', color='green')\nplt.hist(preds_fdia, bins=20, alpha=0.6, label='FDIA', color='red')\nplt.xlabel('Predicted Probability')\nplt.ylabel('Frequency')\nplt.title('Predicted Probability Distribution by Class')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.calibration import calibration_curve\nimport matplotlib.pyplot as plt\n\n# Compute calibration curve: fraction of positives vs mean predicted probability\nprob_true, prob_pred = calibration_curve(y_test, y_pred_prob, n_bins=10)\n\nplt.figure(figsize=(8, 5))\nplt.plot(prob_pred, prob_true, marker='o', label='Calibration Curve')\nplt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')\nplt.xlabel('Mean Predicted Probability')\nplt.ylabel('Fraction of Positives')\nplt.title('Calibration Curve')\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Optionally suppress specific FutureWarnings from Seaborn\nwarnings.filterwarnings(\"ignore\", message=\".*use_inf_as_na.*\", module=\"seaborn\")\nwarnings.filterwarnings(\"ignore\", message=\".*When grouping with a length-1 list-like.*\", module=\"seaborn\")\n\n# Replace infinities with NaN before plotting\ndf_plot = measurement_df.replace([np.inf, -np.inf], np.nan).copy()\ndf_plot['Label'] = np.where(y==1, 'FDIA', 'Normal')\n\n# Plot distributions for each feature\nfeatures_to_plot = ['pRES', 'value', 'p_t']\nplt.figure(figsize=(15, 4))\nfor idx, feature in enumerate(features_to_plot):\n    plt.subplot(1, len(features_to_plot), idx+1)\n    sns.histplot(data=df_plot, x=feature, hue='Label', element='step', stat=\"density\", common_norm=False)\n    plt.title(f\"Distribution of {feature}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate the correlation matrix for the measurement features\nfeatures_to_plot = ['pRES', 'value', 'p_t']\ncorr_matrix = measurement_df[features_to_plot].corr()\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\nplt.title('Correlation Heatmap of Measurement Features')\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}